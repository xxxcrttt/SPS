{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual\n",
    "from torch.distributions import Bernoulli\n",
    "from torch.distributions import MultivariateNormal as MvNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Lecture 3: Bayesian inference for unsupervised learning </h1>\n",
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\Q}{\\mathbf{Q}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\Uniform}{{\\rm Uniform}\\b}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simple example: A biased coin flip</h2>\n",
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "$$\n",
    "The simplest example is a Bernoulli distribution with probability $p$, i.e. a biased coin flip.  We can sample from this distribution using the PyTorch distributions library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fe4c103a3b45bd88d1031c98764e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='p', max=1.0, step=0.05), Button(description='Run Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def sample_bernoulli(p):\n",
    "    return Bernoulli(probs=p).sample((10,))\n",
    "interact_manual(sample_bernoulli, p=FloatSlider(value=0.5, min=0, max=1, step=0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can also compute the probability of a sample, under this distribution.  Rembmer that $x$ was distributed according to a Bernoulli,\n",
    "\n",
    "$$P(x| p) = \\Bernoulli{x; p}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\\Bernoulli{x; p} = \\begin{cases} 1-p &\\text{ if } x=0 \\\\ p &\\text{ if } x=1 \\end{cases}$$\n",
    "\n",
    "Thus, we can evaluate the probability of a sample (0. or 1.) under a Bernoulli,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3000)\n",
      "tensor(0.7000)\n"
     ]
    }
   ],
   "source": [
    "bern = Bernoulli(probs=0.7)\n",
    "print(bern.log_prob(0.).exp())\n",
    "print(bern.log_prob(1.).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: PyTorch only provides the log_prob method, because this is much more numerically stable when doing deep-learning.  We therefore have to exponentiate to get back the probability.\n",
    "\n",
    "<h3> Maximum likelihood fitting of a Bernoulli </h3>\n",
    "\n",
    "We begin by specifying the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = t.tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood, i.e. the probability of all the data, treated as a function of the parameter, $p$, is\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x| p} = \\prod_\\lambda \\P{x_\\lambda| p}\n",
    "\\end{align}\n",
    "\n",
    "But these numbers rapidly shrink (and become too small for standard floating points).  Instead, we work with the log-likelihood,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(p) = \\sum_\\lambda \\log \\P{x_\\lambda| p}\n",
    "\\end{align}\n",
    "\n",
    "Importantly, the log-likelihood is considered a function of the parameter(s), $p$, but not the data, which is considered fixed.\n",
    "\n",
    "The goal is to find $\\ph$ which maximizes $\\L(p)$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\ph = \\argmax_{p} \\L(p)\n",
    "\\end{align}\n",
    "\n",
    "Lets find $ \\ph $!  We can code-up the log-likelihood using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7492c307c7ed4706a864043a5e4649a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='p', max=0.99, min=0.01, step=0.01), Output()), _dom…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def log_likelihood(p):\n",
    "    return Bernoulli(probs=p).log_prob(xs).sum(-1, keepdim=True)\n",
    "\n",
    "interact(log_likelihood, p=FloatSlider(min=0.01, max=0.99, step=0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the log-likelihood,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0100],\n",
       "        [0.0200],\n",
       "        [0.0300],\n",
       "        [0.0400],\n",
       "        [0.0500],\n",
       "        [0.0600]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = t.linspace(0.01, 0.99, 99)[:, None]\n",
    "print(ps.shape)\n",
    "ps[:6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([99, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lls = log_likelihood(ps)\n",
    "lls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6581f814865549ba9313fee64add72b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ps, lls)\n",
    "ax.set_xlabel(\"p\")\n",
    "ax.set_ylabel(\"log-likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple cases like this, we could just use the above plot to find the value of $ p $ with the maximal log-likelihood.\n",
    "\n",
    "But in more complicated cases with many more parameters, this is no longer possible.\n",
    "\n",
    "Instead, the first approach is to try to find an analytic expression for the $p$ with the maximal log-probability.\n",
    "\n",
    "Remember,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(p) &= \\sum_\\lambda \\log \\P{x_\\lambda| p}\\\\\n",
    "  \\L(p) &= \\b{\\sum_\\lambda x_\\lambda} \\log p + \\b{N - \\sum_\\lambda x_\\lambda} \\log (1-p)\n",
    "\\end{align}\n",
    "\n",
    "And solve for where the gradient is zero,\n",
    "\n",
    "\\begin{align}\n",
    "  0 &= \\at{\\dd[\\L(p)]{p}}_{p=\\ph}\\\\\n",
    "  0 &= \\frac{\\sum_\\lambda x_\\lambda}{\\ph} - \\frac{N - \\sum_\\lambda x_\\lambda}{1-\\ph}\\\\\n",
    "  \\frac{\\sum_\\lambda x_\\lambda}{\\ph} &= \\frac{N - \\sum_\\lambda x_\\lambda}{1-\\ph}\\\\\n",
    "  (1-\\ph) \\sum_\\lambda x_\\lambda &= \\ph \\b{N - \\sum_\\lambda x_\\lambda}\\\\\n",
    "  \\sum_\\lambda x_\\lambda &= \\ph N\\\\\n",
    "  \\ph &= \\tfrac{1}{N} \\sum_i x_i\n",
    "\\end{align}\n",
    "\n",
    "The maximum-likelihood probability is the empirical mean of the data points.  This is sensible, but not at all obvious: we needed to go through the derivation!\n",
    "\n",
    "Using this derivation, we can write a function that automatically fits a Bernoulli distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bernoulli(xs):\n",
    "    p = xs.sum() / xs.shape[-1]\n",
    "    return Bernoulli(probs=p)\n",
    "\n",
    "fitted_bernoulli = fit_bernoulli(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "such that samples from the fitted Bernoulli look like the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_bernoulli.sample((12,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Bayesian fitting of a Bernoulli </h3>\n",
    "\n",
    "Maximum likelihood works well when we have a reasonable number of datapoints.\n",
    "\n",
    "But what about when we have very little data (e.g. we might have many biased coins, and be able to toss each one a few times).\n",
    "\n",
    "In particular, lets say we have a population of coins, whose probabilities are drawn from a uniform distribution,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{p} &= \\Uniform{p; 0, 1}\n",
    "\\end{align}\n",
    "\n",
    "we take out one coin, toss it twice, and get two zeros,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = t.tensor([0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we say about $p$?\n",
    "\n",
    "Maximum likelihood would tell us that $ \\ph = 0 $,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bernoulli(probs: 0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_bernoulli(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a bit strange, because $ p $ could have been quite large, but coincidentally, our two coin-tosses happened to be zeros.\n",
    "\n",
    "To understand this in a bit more depth, we can run a simulation.\n",
    "\n",
    "We start by drawing a large number of $ p $'s, from the uniform prior,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e101f2eac95845e1831edbb7216ea8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 10**5\n",
    "ps = t.rand(N)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"p\")\n",
    "ax.set_ylabel(\"P(p)\")\n",
    "ax.hist(ps, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the values on the x-axis are probability *densities*, not probabilities.\n",
    "\n",
    "The probability of being in any given bin is:\n",
    "\\begin{align}\n",
    "  \\int_{p_0}^{p_0 + \\delta} dp \\; \\P{p} \\approx \\delta \\P{p}\n",
    "\\end{align}\n",
    "i.e. the bin-width times the probability density.\n",
    "\n",
    "Here, we have $20$ bins, with probability density $1$, and bin width $\\delta = 1/20$.\n",
    "\n",
    "The probability of being in any 1 bin is therefore $\\P{p} \\times \\delta = 1 \\times 1/20 = 1/20$, and adding up the $20$ bins gives us $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each of these $ p $'s, we toss a couple of coins,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = Bernoulli(ps).sample((2,))\n",
    "xs\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we filter out only those values of $ p $ which actually gave us two zeros,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True, False,  ..., False,  True, False])\n",
      "tensor(33303)\n",
      "torch.Size([33303])\n"
     ]
    }
   ],
   "source": [
    "all_zeros = (xs==0.).all(0)\n",
    "print(all_zeros)\n",
    "print(all_zeros.sum())\n",
    "ps_all_zeros = ps[all_zeros]\n",
    "print(ps_all_zeros.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot those $ p $'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588f8bbb708247919f304003112ec936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"p\")\n",
    "ax.set_ylabel(\"P(p|x=[0,0])\")\n",
    "ax.hist(ps_all_zeros, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense: we expect the probability to be small, because when we flipped the coin, we observed two zeros.\n",
    "\n",
    "But the probability could still be large, with the two zeros we observed being coincidences.\n",
    "\n",
    "It turns out this idea (sampling until we get something very close to the data, then looking at the corresponding latents, here the probability, $p$), is a real algorithm, called \"approximate Bayesian computation\" (ABC).\n",
    "\n",
    "But its a terrible idea: don't do it in practice unless you really have to.  For any non-trivial dataset, you will be waiting a very, very long time before random sampling produces something \"sufficient close\".\n",
    "\n",
    "Instead, can we do exact Bayesian inference to directly compute the distribution $\\P{p| \\x}$.\n",
    "\n",
    "The answer is yes!\n",
    "\n",
    "In particular, the law of joint probability tells us that we can write the joint, $\\P{\\x, p}$ in two equivalent forms,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x, p} &= \\P{\\x| p} \\P{p} = \\P{p| \\x} \\P{\\x}\n",
    "\\end{align}\n",
    "\n",
    "The first form $\\P{\\x| p} \\P{p}$, is the standard one, and we can readily compute it given the expressions above.  The second form, $\\P{p| \\x} \\P{\\x}$ is a bit more unusual: it isn't immediately obvious how we can compute these terms.\n",
    "\n",
    "Nonetheless, we can rearrange to compute the term we're interested in,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{p| \\x} &= \\frac{\\P{\\x| p} \\P{p}}{\\P{\\x}} \\propto \\P{\\x| p} \\P{p}\\\\\n",
    "  \\log \\P{p| \\x} &= \\L\\b{p} + \\log \\P{p} + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "The proportionality arises because we take data, $\\x$, to be fixed, so we only care about parameter, $p$ dependence (as long as we make sure the distribution normalizes!\n",
    "\n",
    "Lets do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c69b61cf544750a9b77e192b6533be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = t.Tensor([0., 0.])\n",
    "\n",
    "def log_prior(p):\n",
    "    return 0.\n",
    "\n",
    "def log_likelihood(p):\n",
    "    return Bernoulli(p).log_prob(xs).sum(-1, keepdim=True)\n",
    "\n",
    "ps = t.linspace(0.005, 0.995, 100).view(-1, 1)\n",
    "dp = ps[1] - ps[0]\n",
    "    \n",
    "unnorm_log_posterior = log_prior(ps) + log_likelihood(ps)\n",
    "unnorm_posterior = unnorm_log_posterior.exp()\n",
    "norm_posterior = unnorm_posterior / (dp * unnorm_posterior.sum())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"p\")\n",
    "ax.set_ylabel(\"P(p|x=[0,0])\")\n",
    "ax.hist(ps_all_zeros, density=True);\n",
    "ax.plot(ps, norm_posterior);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to analytically compute the distribution over $p$ for a coin-toss.  But this doesn't give us much additional insight.  Instead, we'll do this below for a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>More interesting examples</h2>\n",
    "In the previous example, a datapoint was either 0 or 1,\n",
    "\n",
    "```\n",
    "X = {0, 1}\n",
    "```\n",
    "\n",
    "but $ x $ could take on almost any other type.  Common examples include:\n",
    "\n",
    "```\n",
    "X = Vector{Float}   # A vector\n",
    "X = Str             # A string\n",
    "X = Image           # An image\n",
    "```\n",
    "\n",
    "Complex, state-of-the-art models for images (GANs) and text (GTP-2) do exactly what we described above.\n",
    "Then, they take a large dataset of images/text do a very large amount of processing, to fit a distribution to that data.\n",
    "Once the distribution has been fitted, they can draw samples from that distribution, that should look like the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Multivariate Normal (Gaussian)</h2>\n",
    "The multivariate Normal is the most important distribution over vectors.\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x| \\m, \\S} &= \\N{\\x; \\m, \\S}\\\\\n",
    "  \\log \\N{\\x; \\m, \\S} &= -\\tfrac{1}{2} \\log \\det{2 \\pi \\S} - \\tfrac{1}{2} \\b{\\x - \\m} \\S^{-1} \\b{\\x-\\m}^T\n",
    "\\end{align}\n",
    "\n",
    "The expectation of the distribution is given by $\\m$ and the covariance is given by $\\S$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\E{\\x} &= \\m\\\\\n",
    "  \\Cov{\\x} &= \\S\n",
    "\\end{align}\n",
    "\n",
    "In code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "mu = t.ones(2)\n",
    "Sigma = t.ones(2,2) + t.eye(2)\n",
    "dist = MvNormal(mu, Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0071,  1.5192],\n",
      "        [ 0.3646,  1.7885],\n",
      "        [-0.0255,  2.4669],\n",
      "        [-0.7367,  2.1331],\n",
      "        [ 2.0999,  0.5540],\n",
      "        [ 2.7106,  1.2354]])\n",
      "torch.Size([1000000, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10**6\n",
    "X = dist.sample((N,))\n",
    "print(X[:6, :])\n",
    "print(X.shape)\n",
    "mu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The mean of the multivariate normal </h3>\n",
    "The expected value of $\\x$ is given by,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m  &= \\E{\\x}\\\\\n",
    "\\end{align}\n",
    "\n",
    "so the empirical mean is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\mh &= \\tfrac{1}{N} \\sum_{\\lambda=1}^N \\x_{\\lambda}\n",
    "\\end{align}\n",
    "\n",
    "and as $ \\x_\\lambda = X_{\\lambda, :} $\n",
    "\n",
    "\\begin{align}\n",
    "  \\hat{\\mu}_i &= \\tfrac{1}{N} \\sum_{\\lambda=1}^N X_{\\lambda i}\n",
    "\\end{align}\n",
    "\n",
    "Now we can check that the true mean is close to the empirical mean,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.])\n",
      "tensor([1.0025, 1.0010])\n"
     ]
    }
   ],
   "source": [
    "print(mu)\n",
    "mh = X.sum(0)/N\n",
    "print(mh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The covariance of the multivariate normal</h3>\n",
    "While the mean characterises the location of the data, the covariance characterises its shape.\n",
    "\n",
    "The covariance is defined as,\n",
    "\n",
    "\\begin{align}\n",
    "  \\S &= \\E{\\b{\\x - \\E{\\x}} \\b{\\x - \\E{\\x}}^T}\n",
    "\\end{align}\n",
    "\n",
    "so the empirical covariance is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Sh &= \\tfrac{1}{N} \\sum_{\\lambda=1}^N \\b{\\x_\\lambda - \\mh} \\b{\\x_\\lambda - \\mh}^T\\\\\n",
    "  \\hat{\\Sigma}_{ij} &= \\tfrac{1}{N} \\sum_{\\lambda=1}^N \\b{X_{\\lambda i} - \\hat{\\mu}_{i}} \\b{X_{\\lambda j} - \\hat{\\mu}_{j}}\\\\\n",
    "  \\Sh &= \\tfrac{1}{N} \\b{\\X - \\mh}^T \\b{\\X - \\mh}\n",
    "\\end{align}\n",
    "\n",
    "Lets check this expression,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 1.],\n",
      "        [1., 2.]])\n",
      "tensor([[2.0025, 1.0018],\n",
      "        [1.0018, 1.9992]])\n"
     ]
    }
   ],
   "source": [
    "print(Sigma)\n",
    "Sh = (X-mh).T @ (X-mh) / N\n",
    "print(Sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some intuition for what the covariance means, we can plot samples from a multivariate normal with the same variance for $x_0$ and $x_1$, but different covariances,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d582a91205494bc8910e9614f1c1045d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='c', max=0.99, min=-0.99, step=0.01), Button(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(c):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_0$\")\n",
    "    ax.set_ylabel(\"$x_1$\")\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    \n",
    "    mu = t.zeros(2)\n",
    "    Sigma = c*t.ones(2, 2) + (1-c)*t.eye(2)\n",
    "    print(\"Sigma =\")\n",
    "    print(Sigma)\n",
    "    dist = MvNormal(mu, Sigma)\n",
    "    xs = dist.sample((10000,))\n",
    "    ax.scatter(xs[:, 0], xs[:, 1])\n",
    "    \n",
    "interact_manual(plot, c=FloatSlider(value=0, min=-0.99, max=0.99, step=0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, consider positive $\\Cov{x_0, x_1} = \\E{x_0 x_1}$.  To get positive covariances, $x_0$ and $x_1$ will tend to have the same sign (either both positive or both negative), so the overall product, $x_0 x_1$ is usually positive.\n",
    "\n",
    "Now, consider negative $\\Cov{x_0, x_1} = \\E{x_0 x_1}$.  To get negative covariances, $x_0$ and $x_1$ will tend to have the different signs (one positive and the other negative), so the overall product, $x_0 x_1$ is usually negative.\n",
    "\n",
    "Now, consider $0 = \\Cov{x_0, x_1} = \\E{x_0 x_1}$.  In this case, $x_0$ and $x_1$ are unrelated.\n",
    "\n",
    "As the covariance approaches the variance, the distribution gets narrower.\n",
    "\n",
    "Until eventually, the only way of achieving,\n",
    "\n",
    "\\begin{align}\n",
    "  1 = \\Var{x_0} = \\Var{x_1} \\approx \\Cov{x_0, x_1} = \\E{x_0 x_1}\n",
    "\\end{align}\n",
    "\n",
    "is by setting,\n",
    "\n",
    "\\begin{align}\n",
    "  x_0 = x_1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Maximum likelihood fitting</h3>\n",
    "\n",
    "Maximum likelihood parameters, $\\m$ and $\\S$, for a multivariate normal is given by the empirical mean, $\\mh$ and covariance $\\Sh$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\mh, \\Sh = \\argmax_{\\m, \\S} \\sb{\\sum_\\lambda \\log \\N{\\x_\\lambda| \\m, \\S}}\n",
    "\\end{align}\n",
    "\n",
    "This seems sensible, but it is not at all obvious!  (Left as an exercise for the reader.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Bayesian fitting of $\\mu$ </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that $\\m$ has a multivariate Normal prior,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\m} &= \\N{\\m; \\0, \\Sprior}\\\\\n",
    "  \\P{\\x_\\lambda| \\m} &= \\N{\\x_\\lambda; \\m, \\Q}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973d87a6cad3427c9275fc1ed9a09098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='N', min=1), Button(description='Run Interact', style=B…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sprior = 2*t.eye(2)\n",
    "Q = t.tensor([[0.3, 0.2], [0.2, 0.5]])\n",
    "Pm = MvNormal(t.zeros(2), Sprior)\n",
    "def sample_X(N):\n",
    "    m = Pm.sample()\n",
    "    X = MvNormal(m, Q).sample((N,))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_0$\")\n",
    "    ax.set_ylabel(\"$x_1$\")\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.scatter(X[:, 0], X[:, 1])\n",
    "    ax.scatter(m[0:1], m[1:2])\n",
    "    \n",
    "interact_manual(sample_X, N=IntSlider(min=1, max=100, value=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our goal is to compute,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\m| \\X} &\\propto \\P{\\m} \\prod_\\lambda \\P{\\x_\\lambda| \\m}\\\\\n",
    "  \\log \\P{\\m| \\X} &= \\log \\P{\\m} + \\sum_\\lambda \\log \\P{\\x_\\lambda| \\m} + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "First, we write out the log-probability under the prior,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\m} &= -\\underbrace{\\tfrac{1}{2} \\m^T \\S^{-1} \\m}_\\text{quadratic} + \\text{const}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we write out the log-likelihood for a single data-point,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\x_\\lambda| \\m} &= -\\tfrac{1}{2} \\b{\\x_\\lambda - \\m}^T \\Q^{-1} \\b{\\x_\\lambda - \\m} + \\text{const}\\\\\n",
    "  \\log \\P{\\x_\\lambda| \\m} &= -\\tfrac{1}{2} \\sb{\\m^T \\Q^{-1} \\m - \\x_\\lambda^T \\Q^{-1} \\m - \\m^T \\Q^{-1} \\x_\\lambda + \\x_\\lambda^T \\Q^{-1} \\x_\\lambda} + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "As,\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{a}^T \\mathbf{b} = \\mathbf{b}^T \\mathbf{a} = \\sum_i a_i b_i,\n",
    "\\end{align}\n",
    "\n",
    "we have,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m^T \\Q^{-1} \\x_\\lambda = \\m^T \\b{\\Q^{-1}}^T \\x_\\lambda = \\b{\\Q^{-1} \\m}^T \\x_\\lambda = \\x_\\lambda^T \\b{\\Q^{-1} \\m} = \\x_\\lambda^T \\Q^{-1} \\m\n",
    "\\end{align}\n",
    "\n",
    "And as $\\x_\\lambda^T \\Q^{-1} \\x_\\lambda$ does not vary with $\\m$, we can incorporate that term into the constant,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\x_\\lambda| \\m} &=\\underbrace{-\\tfrac{1}{2} \\m^T \\Q^{-1} \\m}_\\text{quadratic} + \\underbrace{\\m^T \\Q^{-1} \\x_\\lambda}_\\text{linear} + \\text{const}.\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-prior and log-likelihood are quadratic functions of $\\m$, so we know the posterior must also be Gaussian.\n",
    "\n",
    "The easiest way of identifying the posterior mean and covariance is to name them $\\mpost$ and $\\Spost$, and compute the corresponding quadratic and linear terms,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\m| \\X} &= -\\tfrac{1}{2} \\b{\\m - \\mpost}^T \\Spost^{-1} \\b{\\m - \\mpost} + \\text{const}\\\\\n",
    "  &= -\\tfrac{1}{2} \\sb{\\m^T \\Spost^{-1} \\m - \\mpost^T \\Spost^{-1} \\m - \\m^T \\Spost^{-1} \\mpost + \\mpost^T \\Spost^{-1} \\mpost} + \\text{const}\\\\\n",
    "    &=\\underbrace{-\\tfrac{1}{2} \\m^T \\Spost^{-1} \\m}_\\text{quadratic} + \\underbrace{\\m^T \\Spost^{-1} \\mpost}_\\text{linear} + \\text{const}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\m| \\X} &= \\log \\P{\\m} + \\sum_\\lambda \\log \\P{\\x_\\lambda| \\m} + \\text{const}\\\\\n",
    "  -\\underbrace{\\tfrac{1}{2} \\m^T \\Spost^{-1} \\m}_\\text{quadratic} + \\underbrace{\\m^T \\Spost^{-1} \\mpost}_\\text{linear} &= - \\underbrace{\\tfrac{1}{2} \\m^T \\Sprior^{-1} \\m}_\\text{quadratic} + \\sum_\\lambda \\sb{\\underbrace{-\\tfrac{1}{2} \\m^T \\Q^{-1} \\m}_\\text{quadratic} + \\underbrace{\\m^T \\Q^{-1} \\x_\\lambda}_\\text{linear}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this must be true for all settings of $\\m$, we can separately match the liner and quadratic terms.\n",
    "\n",
    "For the quadratic terms,\n",
    "\\begin{align}\n",
    "  -\\tfrac{1}{2} \\m^T \\Spost^{-1} \\m &= - \\tfrac{1}{2} \\m^T \\Sprior^{-1} \\m - \\sum_\\lambda \\sb{\\tfrac{1}{2} \\m^T \\Q^{-1} \\m}\n",
    "\\end{align}\n",
    "\n",
    "As this must be true for all $\\m$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Spost^{-1} &= \\Sprior^{-1} + \\sum_\\lambda \\Q^{-1}\\\\\n",
    "  \\Spost^{-1} &= \\Sprior^{-1} + N \\Q^{-1}\\\\\n",
    "  \\Spost &= \\b{\\Sprior^{-1} + N \\Q^{-1}}^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the linear terms,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m^T \\Spost^{-1} \\mpost &= \\sum_\\lambda \\m^T \\Q^{-1} \\x_\\lambda\\\\\n",
    "  \\m^T \\Spost^{-1} \\mpost &= N \\m^T \\Q^{-1} \\langle\\x\\rangle\\\\\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "  \\langle \\x \\rangle = \\tfrac{1}{N} \\sum_{\\lambda=1}^N \\x_\\lambda.\n",
    "\\end{align}\n",
    "\n",
    "As this must be true for all $\\m$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Spost^{-1} \\mpost &= N \\Q^{-1} \\langle\\x\\rangle\\\\\n",
    "  \\mpost &= N \\Spost \\Q^{-1} \\langle\\x\\rangle\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity-check, consider a very large number of data points, $N$,\n",
    "\n",
    "In that case, \n",
    "\\begin{align}\n",
    "  \\Sprior^{-1} + N \\Q^{-1} \\approx N \\Q^{-1}\n",
    "  \\Spost = \\b{\\Sprior^{-1} + N \\Q^{-1}}^{-1} \\approx \\b{N \\Q^{-1}}^{-1} = \\tfrac{1}{N} \\Q\n",
    "\\end{align}\n",
    "\n",
    "so,\n",
    "\n",
    "\\begin{align}\n",
    "  \\mpost &\\approx N \\b{\\tfrac{1}{N} \\Q} \\Q^{-1} \\langle\\x\\rangle = \\langle \\x \\rangle\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
