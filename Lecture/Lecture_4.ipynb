{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "$$\n",
    "\n",
    "<h1> Lecture 4: Bayesian regression </h1>\n",
    "\n",
    "When doing regularised maximum likelihood, we used the following objective function,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(\\w) &= \\underbrace{\\log \\N{\\y; \\X \\w, \\sigma^2 \\I}}_{\\log \\P{\\y| \\W, \\x}} \\underbrace{- \\tfrac{1}{2} \\w^T \\La \\w}_{\\log \\P{\\w}}\n",
    "\\end{align}\n",
    "\n",
    "We can interpret this as maximum-a-posteriori inference, if we take,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\w} &= \\log \\N{\\w; \\0, \\Sprior} = -\\tfrac{1}{2} \\det{2 \\pi \\Sprior^{-1}} - \\tfrac{1}{2} \\w^T \\La \\w\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "  \\Sprior = \\La^{-1}.\n",
    "\\end{align}\n",
    "\n",
    "But having a full prior _distribution_ over the weights allows us to do alot more than just having a regulariser.\n",
    "\n",
    "In particular, it means that given inputs, $\\X$, and weights, $\\w$, (and $\\sigma$), we can simulate what the outputs, $\\y$ might look like.\n",
    "\n",
    "Now, we're going to write down a model for the weights,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\w} &= \\N{\\w; \\0, \\Sprior}\n",
    "\\end{align}\n",
    "\n",
    "<h2> Getting insight into the model by sampling functions </h2>\n",
    "Samples of $\\W$ now correspond to functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3b6f2312484c518e228b7fea9df8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='order', max=20, min=1), FloatSlider(value=1.0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cheb(xs, c):\n",
    "    # c is int\n",
    "    coefs = c*[0] + [1]\n",
    "    return np.polynomial.chebyshev.chebval(xs, coefs)\n",
    "\n",
    "def chebX(X, order):\n",
    "    assert (-1 <= X).all() and (X <= 1).all()    \n",
    "    xs = []\n",
    "    for c in range(order):\n",
    "        xs.append(cheb(X, c))\n",
    "    return t.cat(xs, 1)\n",
    "\n",
    "N     = 100  # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "\n",
    "def plot_function(order, Sigma_prior_scale):\n",
    "    xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    Sigma_prior = Sigma_prior_scale * t.eye(order)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    \n",
    "    for i in range(10):\n",
    "        WT = MvNormal(t.zeros(order), Sigma_prior).sample()\n",
    "        Y = chebX(xs, order) @ WT.T\n",
    "        ax.plot(xs, Y)\n",
    "    \n",
    "interact_manual(plot_function, order=IntSlider(min=1, max=20), Sigma_prior_scale=FloatSlider(min=0.01, max=1, value=1, step=0.01));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ed6e024257483596839479b1ff41b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='order', max=20, min=1), FloatSlider(value=1.0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_function(order, Sigma_prior_scale, decay):\n",
    "    xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    Sigma_prior = Sigma_prior_scale * t.eye(order) * t.exp(-decay * t.arange(order))\n",
    "    \n",
    "    print(Sigma_prior.diag())\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    \n",
    "    for i in range(10):\n",
    "        WT = MvNormal(t.zeros(order), Sigma_prior).sample()\n",
    "        Y = chebX(xs, order) @ WT.T\n",
    "        ax.plot(xs, Y)\n",
    "    \n",
    "interact_manual(plot_function, \n",
    "                order=IntSlider(min=1, max=20), \n",
    "                Sigma_prior_scale=FloatSlider(min=0.01, max=1, value=1, step=0.01),\n",
    "                decay=FloatSlider(min=0., max=2., value=1, step=0.01)\n",
    "               );\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful, because we can set/initialize the parameters, $\\S$, by generating functions that look similar to the data (e.g. similar range on $y$, similar wiggliness).  \n",
    "\n",
    "This generative approach gives us a sensible setting for our parameters, and is thus much better than regularisation parameters, which could be anything.\n",
    "\n",
    "We will see later how to fine-tune these values using data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Computing Bayesian posteriors over the weights </h2>\n",
    "\n",
    "Once we have defined a sensible function class, we can perform inference to find the posterior distribution over the weights.\n",
    "\n",
    "This gives us not just a \"best-guess\", output for any given test-point, but a full distribution.\n",
    "\n",
    "The full model is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\w} &= \\N{\\w; \\0, \\Sprior}\\\\\n",
    "  \\P{\\y| \\X, \\w} &= \\N{\\y; \\X \\w, \\sigma^2 \\I}\n",
    "\\end{align}\n",
    "\n",
    "where this is one big multivariate normal distribution over all datapoints jointly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Bayes theorem,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\w| \\X, \\y} &= \\log \\P{\\y| \\X, \\w} + \\log \\P{\\w} + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\y| \\X, \\w} &= - \\tfrac{1}{2\\sigma^2} \\b{\\y - \\X \\w}^T \\b{\\y - \\X \\w} + \\text{const}\\\\\n",
    "  \\log \\P{\\y| \\X, \\w} &= - \\tfrac{1}{2\\sigma^2} \\b{\\y^T \\y - \\w^T \\X^T \\y - \\y^T \\X \\w + \\w^T \\X^T \\X \\w} + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "and as,\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{a}^T \\mathbf{b} = \\mathbf{b}^T \\mathbf{a} = \\sum_i a_i b_i,\n",
    "\\end{align}\n",
    "\n",
    "we have,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\y| \\X, \\w} &= - \\tfrac{1}{2\\sigma^2} \\b{- 2 \\w^T \\X^T \\y + \\w^T \\X^T \\X \\w} + \\text{const}\\\\\n",
    "  \\log \\P{\\w} &= - \\tfrac{1}{2} \\w^T \\Sprior^{-1} \\w + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "thus, the posterior is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\w| \\X, \\y} &=  - \\tfrac{1}{2\\sigma^2} \\w^T \\X^T \\X \\w - \\tfrac{1}{2} \\w^T \\Sprior^{-1} \\w + \\tfrac{1}{\\sigma^2} \\w^T \\X^T \\y + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "Note that the log-posterior is a quadratic function of $\\w$.  Thus, the posterior is multivariate normal.\n",
    "\n",
    "While we could just rearrange the above expression, it turns out to be easier to name the posterior mean, $\\mpost$, and the posterior covariance, $\\Spost$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\w| \\X, \\y} &= \\N{\\w; \\mpost, \\Spost}\\\\\n",
    "  \\log \\P{\\w| \\X, \\y} &= -\\tfrac{1}{2} \\b{\\w - \\mpost}^T \\Spost^{-1} \\b{\\w - \\mpost} + \\text{const}\\\\\n",
    "  \\log \\P{\\w| \\X, \\y} &= -\\tfrac{1}{2} \\w^T \\Spost^{-1} \\w + \\w^T \\Sprior^{-1} \\mpost + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "Now, the two forms for the log-posterior must be equal,\n",
    "\n",
    "\\begin{align}\n",
    "-\\tfrac{1}{2} \\w^T \\Spost^{-1} \\w + \\w^T \\Spost^{-1} \\mpost + \\text{const} &= - \\tfrac{1}{2\\sigma^2} \\w^T \\X^T \\X \\w - \\tfrac{1}{2} \\w^T \\Sprior^{-1} \\w + \\tfrac{1}{\\sigma^2} \\w^T \\X^T \\y + \\text{const}.\n",
    "\\end{align}\n",
    "\n",
    "Critically, because this must be true for all $\\w$, the quadratic and linear terms must be separately equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic terms allow us to identify $\\Spost$,\n",
    "\n",
    "\\begin{align}\n",
    "  -\\tfrac{1}{2} \\w^T \\Spost^{-1} \\w &= - \\tfrac{1}{2\\sigma^2} \\w^T \\X^T \\X \\w - \\tfrac{1}{2} \\w^T \\Sprior^{-1} \\w\\\\\n",
    "  \\w^T \\Spost^{-1} \\w &= \\tfrac{1}{\\sigma^2} \\w^T \\X^T \\X \\w + \\w^T \\Sprior^{-1} \\w\\\\\n",
    "  \\Spost^{-1} &= \\tfrac{1}{\\sigma^2} \\X^T \\X + \\Sprior^{-1}\\\\ \n",
    "  \\Spost^{-1} &= \\tfrac{1}{\\sigma^2} \\b{\\X^T \\X + \\sigma^2 \\Sprior^{-1}}\\\\ \n",
    "  \\Spost &= \\sigma^2 \\b{\\X^T \\X + \\sigma^2 \\Sprior^{-1}}^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear terms allow us to identify $\\mpost$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\w^T \\Spost^{-1} \\mpost &= \\tfrac{1}{\\sigma^2} \\w^T \\X^T \\y\\\\\n",
    "  \\Spost^{-1} \\mpost &= \\tfrac{1}{\\sigma^2} \\X^T \\y\\\\\n",
    "  \\mpost &= \\tfrac{1}{\\sigma^2} \\Spost \\X^T \\y\\\\\n",
    "  \\mpost &= \\b{\\X^T \\X + \\sigma^2 \\Sprior^{-1}}^{-1} \\X^T \\y\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is exactly the same expression as the one we had for regularised linear regression in the previous lecture, with\n",
    "\n",
    "\\begin{align}\n",
    "  \\La &= \\Sprior^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did all that work.  If it just gave us regularised maximum-likelihood, why did we even bother?  Three reasons:\n",
    "\n",
    "<ul>\n",
    "  <li> We can set sensible values for $\\Sprior{}$ and $\\sigma$ by looking at samples from the generative model </li>\n",
    "  <li> We can plot functions corresponding to weights sampled from the posterior </li>\n",
    "  <li> We can compute a posterior distribution over test outputs, taking into account uncertainty due to noise and uncertainty due to limited training data. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Sampling the posterior over functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39b4277497a49c1a66bd3b21a3d04d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def posterior(X, Y, sigma, Sprior):\n",
    "    s2 = sigma**2\n",
    "    Spost = s2 * t.inverse(X.T@X + s2*t.inverse(Sprior))\n",
    "    mpost= 1/s2 * Spost @ (X.T @ Y) \n",
    "    return MvNormal(mpost.T, Spost)\n",
    "\n",
    "N     = 20  # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.1 # output noise\n",
    "rand1 = (1+t.rand(N//2, 1))/3\n",
    "rand2 = -(1+t.rand(N//2, 1))/3\n",
    "X     = t.cat([rand1, rand2], 0)\n",
    "Wtrue = t.tensor([[0.]])\n",
    "Y     = chebX(X, 1) @ Wtrue + sigma*t.randn(N, 1)\n",
    "\n",
    "\n",
    "def plot():\n",
    "    xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    \n",
    "    \n",
    "    for i in range(10):\n",
    "        WsampleT = posterior(chebX(X, 10), Y, sigma, t.eye(10)).sample()\n",
    "        ys = chebX(xs, 10) @ WsampleT.T\n",
    "        ax.plot(xs, ys, alpha=0.5)\n",
    "        \n",
    "    ax.scatter(X, Y)\n",
    "        \n",
    "interact_manual(plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Computing the posterior over a test point </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we plotted the functions corresponding to samples of the weights, but if we have a single test point, this isn't necessarily super-helpful.\n",
    "\n",
    "Instead, we want the posterior mean and variance for that test point, we could get that by sampling many functions, but that's super-slow.\n",
    "\n",
    "Instead,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\yt| \\Xt, \\X, \\y} &= \\int \\mathbf{dw} \\; \\P{\\yt| \\Xt, \\w} \\P{\\w| \\X, \\y}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\yt| \\Xt, \\w} &= \\N{\\yt; \\Xt \\w, \\sigma^2 \\I}\\\\\n",
    "  \\P{\\w| \\X, \\y} &= \\N{\\w; \\mpost, \\Spost}\n",
    "\\end{align}\n",
    "\n",
    "Instead of doing the integral, which is a pain, we can rewrite these expressions as random variables,\n",
    "\n",
    "\\begin{align}\n",
    "  \\yt &= \\Xt \\w + \\sigma \\boldsymbol{\\xi}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\P{\\boldsymbol{\\xi}} = \\N{\\boldsymbol{\\xi}; \\0, \\I}$.\n",
    "\n",
    "The expected value of $\\yt$ is therefore,\n",
    "\n",
    "\\begin{align}\n",
    "  \\E{\\yt} &= \\E{\\Xt \\w + \\sigma \\boldsymbol{\\xi}}\\\\\n",
    "  \\E{\\yt} &= \\Xt \\E{\\w}\\\\\n",
    "  \\E{\\yt} &= \\Xt \\mpost\n",
    "\\end{align}\n",
    "\n",
    "And the variance of $\\yt$ is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Cov{\\yt} &= \\Cov{\\Xt \\w} + \\sigma^2 \\I\\\\\n",
    "  \\Cov{\\yt} &= \\E{\\Xt \\b{\\w - \\mpost} \\b{\\w-\\mpost}^T \\Xt^T} + \\sigma^2 \\I\\\\\n",
    "  \\Cov{\\yt} &= \\Xt \\E{\\b{\\w - \\mpost} \\b{\\w-\\mpost}^T} \\Xt^T + \\sigma^2 \\I \\\\\n",
    "  \\Cov{\\yt} &= \\Xt \\Spost \\Xt^T + \\sigma^2 \\I\n",
    "\\end{align}\n",
    "\n",
    "Thus,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\yt| \\Xt, \\X, \\y} &= \\N{\\yt^T; \\, \\Xt \\mpost, \\, \\Xt \\Spost \\Xt^T + \\sigma^2 \\I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364d7b736dfc4886947bb65dba870fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-4, 4)\n",
    "    \n",
    "PW = posterior(chebX(X, 10), Y, sigma, t.eye(10))\n",
    "mean = chebX(xs, 10) @ PW.mean.T\n",
    "cov = chebX(xs, 10) @ PW.covariance_matrix[0, :, :] @ chebX(xs, 10).T + sigma**2 * t.eye(xs.shape[0])\n",
    "std = cov.diag()[:, None].sqrt()\n",
    "\n",
    "\n",
    "\n",
    "ax.fill_between(xs[:, 0], (mean - 2*std)[:, 0], (mean + 2*std)[:, 0], alpha=0.3, label=\"mean $\\pm$ 2 std\")\n",
    "ax.plot(xs, mean, label=\"mean\")\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> How do we set the \"hyperparameters\" (e.g. $\\sigma$), based on data? </h2>\n",
    "\n",
    "We compute the \"marginal likelihood\",\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\y| \\X} &= \\int \\mathbf{dw} \\; \\P{\\y| \\X, \\w} \\P{\\w}\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\y| \\X, \\w} &= \\N{\\y; \\X \\w, \\sigma^2 \\I}\\\\\n",
    "  \\P{\\w} &= \\N{\\w; \\0, \\Sprior}\n",
    "\\end{align}\n",
    "\n",
    "We can use a similar method as above (except that now things are much easier).\n",
    "\n",
    "\\begin{align}\n",
    "  \\y &= \\X \\w + \\sigma \\boldsymbol{\\xi}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\P{\\boldsymbol{\\xi}} = \\N{\\boldsymbol{\\xi}; \\0, \\I}$.\n",
    "\n",
    "The expected value of $\\y$ is therefore, $\\0$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\E{\\y} &= \\E{\\X \\w + \\sigma \\boldsymbol{\\xi}}\\\\\n",
    "  \\E{\\y} &= \\X \\E{\\w}\\\\\n",
    "  \\E{\\y} &= \\0\n",
    "\\end{align}\n",
    "\n",
    "And the variance of $\\yt$ is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Cov{\\y^T} &= \\Cov{\\X \\w} + \\sigma^2 \\I\\\\\n",
    "  \\Cov{\\y^T} &= \\E{\\X \\w \\w^T \\X^T} + \\sigma^2 \\I\\\\\n",
    "  \\Cov{\\y^T} &= \\X \\E{\\w \\w^T} \\X^T + \\sigma^2 \\I \\\\\n",
    "  \\Cov{\\y^T} &= \\X \\Sprior \\X^T + \\sigma^2 \\I\n",
    "\\end{align}\n",
    "\n",
    "Thus,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\y| \\X} &= \\N{\\y^T; \\, \\0, \\, \\X \\Sprior \\X^T + \\sigma^2 \\I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d8f8543d334719ba06a95389f79c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(-2, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N     = 10  # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.5 # output noise\n",
    "t.manual_seed(1)\n",
    "rand  = t.rand(N, 1)\n",
    "X     = 2*rand - 1\n",
    "Wtrue = t.tensor([[0.2], [0.5]])\n",
    "Y     = chebX(X, 2) @ Wtrue + sigma*t.randn(N, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y)\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854fd99eae9a4fca9100fcbba0ec7eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def marginal_likelihood(X, Y, sigma, order, Sigma_prior_scale):\n",
    "    N = X.shape[0]\n",
    "    Xe = chebX(X, order)\n",
    "    Sprior = Sigma_prior_scale * t.eye(order)\n",
    "    dist = MvNormal(t.zeros(N), Xe @ Sprior @ Xe.T + sigma**2*t.eye(N))\n",
    "    return dist.log_prob(Y.T)\n",
    "\n",
    "log_10_scales = t.linspace(-2.5, 2.5, 100)\n",
    "scales = 10**log_10_scales\n",
    "orders = t.arange(1, 10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"Sigma prior scale\")\n",
    "ax.set_ylabel(\"log marginal likelihood\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylim(-15, -7)\n",
    "\n",
    "for order in range(1, 5):\n",
    "    mls = t.tensor([marginal_likelihood(X, Y, sigma, order, scale) for scale in scales])\n",
    "    ax.plot(scales, mls, label=f\"order={order}\")\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
