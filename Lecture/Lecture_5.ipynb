{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import Normal, Categorical, Bernoulli\n",
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\Categorical}{{\\rm Categorical}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "\\newcommand{\\p}{\\mathbf{p}}\n",
    "\\newcommand{\\l}{\\boldsymbol{\\ell}}\n",
    "\\DeclareMathOperator*{\\softmax}{softmax}\n",
    "\\DeclareMathOperator*{\\sigmoid}{sigmoid}\n",
    "$$\n",
    "\n",
    "<h1> Lecture 5: Classification </h1>\n",
    "\n",
    "Classification is almost exactly the same as regression, except that:\n",
    "<ul>\n",
    "    <li> The outputs, $y$, are discrete class-labels. </li>\n",
    "    <li> Almost all interesting/useful algorithms require iterative solutions </li>\n",
    "</ul>\n",
    "    \n",
    "The same considerations are relevant, including,\n",
    "<ul>\n",
    "    <li> Overfitting </li>\n",
    "    <li> Regularisation </li>\n",
    "    <li> Cross-validation </li>\n",
    "    <li> Bayes (but this is much harder, as there aren't any exact solutions) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites: Bernoulli distribution for two-class classification</h2>\n",
    "Samples from the Bernoulli distribution are either $0$ or $1$, with probability given by the parameter,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y| p} &= \\Bernoulli{y; p} = y p + (1-y) (1-p)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "tensor([0.2000, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "Py = Bernoulli(probs=0.8)\n",
    "print(Py.sample((10,)))\n",
    "print(Py.log_prob(t.tensor([0., 1.])).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logit parameterisation of the Bernoulli distribution and the sigmoid </h3>\n",
    "\n",
    "Working directly with the probabilities turns out to be problematic:\n",
    "<ul>\n",
    "    <li> Probabilities live in a strange range, $0 \\leq p \\leq 1$. </li>\n",
    "    <li> There is a strong risk of numerical underflow, which breaks algorithms such as (stochastic) gradient descent. </li>\n",
    "</ul>\n",
    "\n",
    "Instead, we can also treat the Bernoulli parameter as a logits vector, $\\ell$, defined such that,\n",
    "\n",
    "\\begin{align}\n",
    "  p &= \\sigmoid(\\ell) = \\sigma(\\ell)\\\\\n",
    "  p &= \\frac{1}{1+e^{-\\ell}}\n",
    "\\end{align}\n",
    "\n",
    "Now, no matter what $\\l$ is, the probabilities must lie in the right range, and they are much less likely to underflow.\n",
    "\n",
    "PyTorch allows you to directly use the logit parameterisation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50b352d5049459a81ff29aa1356799a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = t.linspace(-4, 4, 100)\n",
    "ps = t.sigmoid(ls)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$\\ell$\")\n",
    "ax.set_ylabel(\"$p = \\sigma(\\ell)$\")\n",
    "ax.plot(ls, ps);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.8808, 0.1192])\n"
     ]
    }
   ],
   "source": [
    "Py = Bernoulli(logits=-2)\n",
    "print(Py.sample((10,)))\n",
    "print(Py.log_prob(t.tensor([0., 1.])).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites: Categorical distribution for multi-class classification </h2>\n",
    "\n",
    "The Bernoulli distribution takes one parameter and gives the probability of two classes.  But what about multiple classes?\n",
    "\n",
    "Samples from the Categorical distribution are integers $0 \\leq y < K$, with probability given explicity by a length $K$ vector of, $\\p$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y| \\p} &= \\Categorical{y; \\p} = p_y\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5, 2, 3, 7, 3, 7, 3, 3, 8, 5, 0, 4, 2, 0, 7, 2, 1, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "#A uniform Categorical distribution,\n",
    "Py = Categorical(probs=t.ones(10)/10)\n",
    "print(Py.sample((20,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = t.arange(10)\n",
    "print(ys)\n",
    "Py.log_prob(ys).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 4, 0, 0, 5, 0, 4, 0, 4, 3, 4, 0, 0, 0, 3, 4, 2, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A non-uniform Categorical distribution,\n",
    "p = t.tensor([0.5, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "Py = Categorical(probs=p)\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(t.arange(6)).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logit parameterisation of the Categorical distribution and the softmax </h3>\n",
    "\n",
    "Working directly with the probabilities turns out to be problematic:\n",
    "<ul>\n",
    "    <li> Probabilities live in a strange range, $0 \\leq p_i \\leq 1$. </li>\n",
    "    <li> Probabilities must sum to $1$. </li>\n",
    "    <li> There is a strong risk of numerical underflow, which breaks algorithms such as (stochastic) gradient descent. </li>\n",
    "</ul>\n",
    "\n",
    "Instead, we can also treat the Categorical parameter as a logits vector, $\\l$, defined such that,\n",
    "\n",
    "\\begin{align}\n",
    "  \\p &= \\softmax(\\l)\\\\\n",
    "  p_i &= \\frac{e^{\\ell_i}}{\\sum_{j=0}^{K-1} e^{\\ell_j}}\n",
    "\\end{align}\n",
    "\n",
    "Now, no matter what $\\l$ is, the probabilities must lie in the right range, they must normalize, and they are much less likely to underflow.\n",
    "\n",
    "PyTorch allows you to directly use the logit parameterisation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 2, 2, 8, 0, 2, 1, 2, 8, 8, 1, 1, 8, 0, 0, 2, 5, 6, 7])\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000])\n"
     ]
    }
   ],
   "source": [
    "# Uniform Categorical\n",
    "Py = Categorical(logits = t.ones(10))\n",
    "print(Py.sample((20,)))\n",
    "print(Py.log_prob(t.arange(10)).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 2, 1, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 3, 2, 2, 1, 0, 0])\n",
      "tensor([6.4391e-01, 2.3688e-01, 8.7143e-02, 3.2058e-02, 1.0754e-05])\n"
     ]
    }
   ],
   "source": [
    "# Non-uniform Categorical\n",
    "l = t.tensor([1., 0., -1., -2., -10.])\n",
    "Py = Categorical(logits = l)\n",
    "print(Py.sample((20,)))\n",
    "print(Py.log_prob(t.arange(5)).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Formalising maximum-likelihood supervised learning </h2>\n",
    "\n",
    "For linear regression, the goal was to predict the distribution over a floating-point $y_\\lambda$ based on a given value for $\\x_\\lambda$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &= \\N{y_\\lambda; \\x_\\lambda \\cdot \\w, \\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "In classification, the goal is similar: we again want to predict a distribution over $y_\\lambda$, based on an input, $\\x_\\lambda$.  The only difference is that here, $y_\\lambda$ is an integer from $0$ to $K-1$.  Thus, when we predict a probability over $y_\\lambda$ it either needs to be Bernoulli (for two classes), or categorical (if there can be more classes,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &= \\Bernoulli{y_\\lambda; \\sigma\\b{\\x_\\lambda \\cdot \\w}}\\\\\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &= \\Categorical{y_\\lambda; \\softmax\\b{\\x_\\lambda \\W}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the \"Iris\" dataset, which is about classifying flowers based on features such as Petal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d116355e3342f1a650c47172d97b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "_X = t.tensor(iris['data'][50:, [0, 2]]).float()\n",
    "_Y = (t.tensor(iris['target'][50:])-1).float()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(_X[:, 0], _X[:, 1], c=_Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test (we shuffle the data first, because the classes are ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(0)\n",
    "perm = t.randperm(_X.shape[0])\n",
    "X = _X[perm, :]\n",
    "X = t.cat([X, t.ones(X.shape[0], 1)], -1)\n",
    "Y = _Y[perm][:, None]\n",
    "\n",
    "X_train = X[:70, :]\n",
    "Y_train = Y[:70, :]\n",
    "X_test  = X[70:, :]\n",
    "Y_test  = Y[70:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some more quicker iterative algorithms.  But they don't add much understanding.  So instead, we use PyTorch-magic to do gradient-descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48.974510192871094\n",
      "-20.449899673461914\n",
      "-17.061647415161133\n",
      "-15.644773483276367\n",
      "-14.787422180175781\n",
      "-14.168648719787598\n",
      "-13.676431655883789\n",
      "-13.261651992797852\n",
      "-12.899398803710938\n",
      "-12.575631141662598\n",
      "-12.281757354736328\n",
      "-12.01214599609375\n",
      "-11.762899398803711\n",
      "-11.53117561340332\n",
      "-11.31480884552002\n",
      "-11.112078666687012\n",
      "-10.92162799835205\n",
      "-10.742270469665527\n",
      "-10.573012351989746\n",
      "-10.413020133972168\n",
      "-10.261512756347656\n",
      "-10.117838859558105\n",
      "-9.981404304504395\n",
      "-9.851656913757324\n",
      "-9.728116989135742\n",
      "-9.610358238220215\n",
      "-9.49797534942627\n",
      "-9.39061450958252\n",
      "-9.287946701049805\n",
      "-9.189657211303711\n",
      "-9.09546184539795\n",
      "-9.005102157592773\n",
      "-8.918354988098145\n",
      "-8.835001945495605\n",
      "-8.754850387573242\n",
      "-8.677701950073242\n",
      "-8.603403091430664\n",
      "-8.53178882598877\n",
      "-8.46267032623291\n",
      "-8.395984649658203\n",
      "-8.331559181213379\n",
      "-8.269283294677734\n",
      "-8.20904541015625\n",
      "-8.150764465332031\n",
      "-8.094314575195312\n",
      "-8.039603233337402\n",
      "-7.9865803718566895\n",
      "-7.935168266296387\n",
      "-7.88528299331665\n",
      "-7.836832046508789\n"
     ]
    }
   ],
   "source": [
    "W = t.randn((3,1), requires_grad=True)/100\n",
    "\n",
    "for i in range(50000):\n",
    "    L = Bernoulli(logits=X_train@W).log_prob(Y_train).sum()\n",
    "    \n",
    "    dW = t.autograd.grad(outputs=L, inputs=(W,))[0]\n",
    "    if 0==i % 1000:\n",
    "        print(L.item())\n",
    "    W.data += 0.001*dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the classification boundary,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ad381b02874d3e88926b41049aa91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(_X[:, 0], _X[:, 1], c=_Y);\n",
    "\n",
    "x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "\n",
    "x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "xs = t.stack([x0s, x1s, t.ones(500, 500)], -1)\n",
    "\n",
    "ps = Bernoulli(logits=xs@W.detach()).probs[:, :, 0]\n",
    "ax.contour(x0s, x1s, 0.5<ps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the classification-error,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct\n",
      "67/70 = 95.71428680419922%\n",
      "Test correct\n",
      "28/30 = 93.33333587646484%\n"
     ]
    }
   ],
   "source": [
    "def class_error(X, Y):\n",
    "    Py = Bernoulli(logits=X@W)\n",
    "    pred = 0.5 < Py.probs\n",
    "    N_correct = (pred == Y).sum()\n",
    "    print(f\"{N_correct}/{X.shape[0]} = {100.*N_correct/X.shape[0]}%\")\n",
    "    \n",
    "print(\"Training correct\")\n",
    "class_error(X_train, Y_train)\n",
    "\n",
    "print(\"Test correct\")\n",
    "class_error(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the classifier performs really well on training data, but less well on test data.\n",
    "\n",
    "Indicates evidence of overfitting (but too few datapoints to draw strong conclusions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Overfitting in classification</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5843ff58974f55a99f86f105802098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='order', max=30, min=2), Button(description='Run Interactâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cheb(xs, c):\n",
    "    # c is int\n",
    "    coefs = c*[0] + [1]\n",
    "    return np.polynomial.chebyshev.chebval(xs, coefs)\n",
    "def chebX(X, order):\n",
    "    assert (-1 <= X).all() and (X <= 1).all()\n",
    "    \n",
    "    xs = []\n",
    "    for c in range(order):\n",
    "        xs.append(cheb(X, c))\n",
    "    return t.cat(xs, 1)\n",
    "t.manual_seed(0)\n",
    "N = 100\n",
    "X = 2*t.rand(N, 1)-1\n",
    "W_true = t.tensor([[4.]])\n",
    "Y = Bernoulli(logits=X@W_true).sample()\n",
    "\n",
    "def plot(order):\n",
    "    Xe= chebX(X, order)\n",
    "    W = t.zeros((order, 1), requires_grad=True)\n",
    "\n",
    "    for i in range(15000):\n",
    "        L = Bernoulli(logits=Xe@W).log_prob(Y).sum()\n",
    "        if 0==i % 2000:\n",
    "            print(L.item())\n",
    "        dW = t.autograd.grad(outputs=L, inputs=(W,))[0]\n",
    "        W.data += 0.001*dW\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"probability / $y$\")\n",
    "    ax.scatter(X, Y)\n",
    "    xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    ps_fitted = Bernoulli(logits=chebX(xs, order)@W).probs.detach()\n",
    "    ps_true = Bernoulli(logits=xs@W_true).probs.detach()\n",
    "    ax.plot(xs, ps_fitted, label=\"fitted probability\")\n",
    "    ax.plot(xs, ps_true, label=\"true probability\")\n",
    "    ax.legend()\n",
    "    \n",
    "interact_manual(plot, order=IntSlider(min=2, max=30));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the same techniques to control overfitting:\n",
    "<ul>\n",
    "    <li> Regularisation </li>\n",
    "    <li> Cross-validation (either on the number of classification errors for test points, or on the test-log-likelihood </li>\n",
    "    <li> Bayesian inference (though its much harder here because the posterior over the weights doesn't have an analytic form)\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Simpler, heuristic methods for classification </h2>\n",
    "\n",
    "We've seen a bunch of complex methods, and implementing them is often quite involved.  Is there anything simpler?\n",
    "\n",
    "The answer is yes!\n",
    "\n",
    "<h3> K-nearest neighbour </h3>\n",
    "\n",
    "One approach is to look at the nearby datapoints.  If the nearby points come from one class, then the chances are that our datapoint come from the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b39090f357942508efe276b907aaa38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='K', max=11, min=1, step=2), Button(description='Run Inteâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = t.tensor(iris['data'][50:, [0, 2]]).float()\n",
    "Y = (t.tensor(iris['target'][50:])-1)[:, None]\n",
    "\n",
    "def plot(K):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "    ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=Y[:, 0]);\n",
    "\n",
    "    x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "    x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "\n",
    "    x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "    xs = t.stack([x0s, x1s], -1)\n",
    "\n",
    "    X_exp = X[:, None, None, :]\n",
    "    \n",
    "    dist2 = ((X_exp - xs)**2).sum(-1)\n",
    "    elems = (-dist2).topk(K, dim=0).indices\n",
    "    pred = (0.5<Y[elems, 0].float().mean(0)).float()\n",
    "    ax.contour(x0s, x1s, pred)\n",
    "    \n",
    "interact_manual(plot, K=IntSlider(min=1, max=11, step=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cross-validation </h3>\n",
    "\n",
    "Choose $K$ using cross-validation.  Note that leave-one-out cross validation is very suitable here, as we make a prediction for each point, based on its neighbours, in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Weighted-nearest neighbour </h3>\n",
    "\n",
    "This approach is to take a weighted average of nearby datapoints,\n",
    "\n",
    "\\begin{align}\n",
    "  p_{\\lambda} &= \\frac{\\sum_{\\lambda'}k(\\x_\\lambda, \\x_{\\lambda'}) \\delta_{1, y_{\\lambda'}}}{\\sum_{\\lambda''} k(\\x_\\lambda, \\x_{\\lambda''})}\n",
    "\\end{align}\n",
    "\n",
    "where we might use a squared-exponential kernel/weights (but many other choices are available),\n",
    "\n",
    "\\begin{align}\n",
    "  k(\\x_\\lambda, \\x_{\\lambda'}) = e^{-(\\x_\\lambda - \\x_{\\lambda'})^2/(2 b)}\n",
    "\\end{align}\n",
    "\n",
    "where $b$ is a bandwidth/lengthscale parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ad3eefbe4d412fa0f2e6441ab9f2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.15, description='bandwidth', max=1.0, min=0.15, step=0.01), Button(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(bandwidth):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "    ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=Y[:, 0]);\n",
    "\n",
    "    x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "    x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "\n",
    "    x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "    xs = t.stack([x0s, x1s], -1)\n",
    "\n",
    "    X_exp = X[:, None, None, :]\n",
    "\n",
    "    dist2 = ((X_exp - xs)**2).sum(-1)\n",
    "    ws = t.exp(-dist2/(2*bandwidth**2))\n",
    "    print(ws.shape)\n",
    "    pred = (ws*Y[:, :, None]).sum(0) / ws.sum(0)\n",
    "    ax.contour(x0s, x1s, 0.5<pred);\n",
    "    \n",
    "interact_manual(plot, bandwidth=FloatSlider(min=0.15, max=1., step=0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cross-validation </h3>\n",
    "\n",
    "Choose bandwidth using cross-validation.  Note that leave-one-out cross validation is very suitable here, as we make a prediction for each point, based on its neighbours, in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Nearest centroids </h2>\n",
    "\n",
    "Saw this in the lab.  Here we compute the center of the inputs for each class, and to classify a new input point, we ask which cluster-center it is closest to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03fd99440c14e40814747a62b4745ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y[:, 0]);\n",
    "\n",
    "mu0 = (X*(1-Y)).sum(0) / (1-Y).sum()\n",
    "mu1 = (X*Y).sum(0) / Y.sum()\n",
    "\n",
    "ax.scatter(t.tensor([mu0[0], mu1[0]]), t.tensor([mu0[1], mu1[1]]), c=t.tensor([0., 1.]), s=200, marker=\"+\")\n",
    "\n",
    "x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "\n",
    "x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "xs = t.stack([x0s, x1s], -1)\n",
    "\n",
    "d0 = ((xs - mu0)**2).sum(-1)\n",
    "d1 = ((xs - mu1)**2).sum(-1)\n",
    "pred = d1 < d0 \n",
    "\n",
    "ax.contour(x0s, x1s, pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Using Bayes theorem to do classification </h2>\n",
    "\n",
    "Nearest centroid works quite poorly, because it only takes into account the mean of the classes, not their shape.\n",
    "\n",
    "To take into account the shape of these distributions, it turns out that we can use Bayes theorem to compute a probability distribution over the class-label (in contrast to previously, where we used Bayes theorem to compute a distribution over the parameters for linear regression).\n",
    "\n",
    "In particular, above we directly learned weights that map directly from the data point to a distribution over class labels.  Here, we separately learn a distribution over data-points for a single class, $\\P{x_\\lambda| y_\\lambda}$.  And we could in principle use this distribution to generate $\\x_\\lambda$ that look like the input points.\n",
    "\n",
    "We can also use Bayes theorem to give us the probability of a class, conditioned on a data-point,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &\\propto \\P{y_\\lambda} \\P{\\x_\\lambda| y_\\lambda}.\n",
    "\\end{align}\n",
    "\n",
    "The simplest example is to take,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_\\lambda| y_\\lambda} &= \\N{\\x_\\lambda; \\m_{y_\\lambda}, \\sigma^2 \\I}.\n",
    "\\end{align}\n",
    "\n",
    "This ends up being equivalent to nearest-centroid!\n",
    "\n",
    "In particular, consider a uniform prior,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda} = 1/K\n",
    "\\end{align}\n",
    "\n",
    "so the posterior becomes proportional to the likelihood,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &\\propto \\P{\\x_\\lambda| y_\\lambda} = \\N{\\x_\\lambda; \\m_{y_\\lambda}, \\sigma^2 \\I}.\n",
    "\\end{align}\n",
    "\n",
    "And the prediction is the class with the highest probability density.\n",
    "\n",
    "Converting to the log-domain (remembering that the log-transform is monotonically increasing, so it doesn't change the ordering),\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{y_\\lambda| \\x_\\lambda} &= \\log \\N{\\x_\\lambda; \\m_{y_\\lambda}, \\sigma^2 \\I} + \\text{const}\\\\\n",
    "  &= -\\tfrac{1}{2\\sigma^2} \\b{\\x_\\lambda - \\m_{y_\\lambda}}^2 + \\text{const}\\\\\n",
    "\\end{align}\n",
    "\n",
    "So the highest-probability class is the one with the smallest distance between the input, $\\x_\\lambda$, and the mean for that class, $\\m_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critically, once we have this probabilistic representation, we get a recipe for doing a better job, by taking into account the shape of the input distributions.  In particular, we can fit multivariate normals to the input points,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50c52fcff1748b8afae82986549d3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y[:, 0]);\n",
    "\n",
    "X0 = X[Y[:, 0]==0, :]\n",
    "X1 = X[Y[:, 0]==1, :]\n",
    "\n",
    "mu0 = X0.mean(0)\n",
    "mu1 = X1.mean(0)\n",
    "\n",
    "C0 = (X0-mu0).t() @ (X0-mu0) / X0.shape[0]\n",
    "C1 = (X1-mu1).t() @ (X1-mu1) / X1.shape[0]\n",
    "\n",
    "N0 = MvNormal(mu0, C0)\n",
    "N1 = MvNormal(mu1, C1)\n",
    "\n",
    "\n",
    "x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "xs = t.stack([x0s, x1s], -1)\n",
    "\n",
    "ax.contour(x0s, x1s, N0.log_prob(xs).exp())\n",
    "ax.contour(x0s, x1s, N1.log_prob(xs).exp());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861ce33b32e446cabf955eeb4081d610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y[:, 0]);\n",
    "\n",
    "x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "xs = t.stack([x0s, x1s], -1)\n",
    "\n",
    "ax.contour(x0s, x1s, 0<(N1.log_prob(xs) - N0.log_prob(xs)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3f3a0fef384681a312143460290896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X0 = t.randn(50, 2)\n",
    "X1 = t.randn(50, 2) / 5\n",
    "\n",
    "Y0 = t.zeros(50, 1)\n",
    "Y1 = t.ones(50, 1)\n",
    "\n",
    "X = t.cat([X0, X1], 0)\n",
    "Y = t.cat([Y0, Y1], 0)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y[:, 0]);\n",
    "\n",
    "X0 = X[Y[:, 0]==0, :]\n",
    "X1 = X[Y[:, 0]==1, :]\n",
    "\n",
    "mu0 = X0.mean(0)\n",
    "mu1 = X1.mean(0)\n",
    "\n",
    "C0 = (X0-mu0).t() @ (X0-mu0) / X0.shape[0]\n",
    "C1 = (X1-mu1).t() @ (X1-mu1) / X1.shape[0]\n",
    "\n",
    "N0 = MvNormal(mu0, C0)\n",
    "N1 = MvNormal(mu1, C1)\n",
    "\n",
    "\n",
    "x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "xs = t.stack([x0s, x1s], -1)\n",
    "\n",
    "ax.contour(x0s, x1s, 0<(N1.log_prob(xs) - N0.log_prob(xs)));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
