{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import Bernoulli, Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "$$\n",
    "\n",
    "<h1> Question sheet 2: classification and clustering </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 1: Classification: maximum likelihood </h3>\n",
    "\n",
    "Find the best model on the following test-set, using the log-likelihood as the objective.\n",
    "\n",
    "```  \n",
    " x_0    x_1   y\n",
    "-2.1   -3.2   0\n",
    "-3.4   -1.2   0\n",
    " 1.2    3.6   1\n",
    "-0.1    0.8   1\n",
    "```\n",
    "The models are,\n",
    "1. $l = x_0 + x_1$\n",
    "2. $l = 4 x_0 + 0.01 x_1$\n",
    "3. $l = 100 x_0 + 100 x_1$\n",
    "\n",
    "where $l$ is the logits-value for $y=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = t.tensor([\n",
    "    [-2.1, -3.2],\n",
    "    [-3.4, -1.2],\n",
    "    [ 1.2,  3.6],\n",
    "    [-0.1,  0.8]\n",
    "])\n",
    "\n",
    "y = t.tensor([\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [1.],\n",
    "    [1.]\n",
    "])\n",
    "\n",
    "W = t.tensor([\n",
    "    [1, 4, 10],\n",
    "    [1, 0.01, 10]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Compute the logits-value for each datapoint,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "l_datapoints = t.zeros(4, 3)\n",
    "#All datapoints, for first model,\n",
    "l[0, 0] = 1.0*(-2.1) + 1.0*(-3.2)\n",
    "l[1, 0] = 1.0*(-3.4) + 1.0*(-1.2)\n",
    "l[2, 0] = 1.0*( 1.2) + 1.0*( 3.6)\n",
    "l[3, 0] = 1.0*(-0.1) + 1.0*( 0.8)\n",
    " \n",
    "l[0, 1] = 4.0*(-2.1) + 0.01*(-3.2)\n",
    "l[1, 1] = 4.0*(-3.4) + 0.01*(-1.2)\n",
    "l[2, 1] = 4.0*( 1.2) + 0.01*( 3.6)\n",
    "l[3, 1] = 4.0*(-0.1) + 0.01*( 0.8)\n",
    "\n",
    "l[0, 2] = 10.0*(-2.1) + 10.0*(-3.2)\n",
    "l[1, 2] = 10.0*(-3.4) + 10.0*(-1.2)\n",
    "l[2, 2] = 10.0*( 1.2) + 10.0*( 3.6)\n",
    "l[3, 2] = 10.0*(-0.1) + 10.0*( 0.8)\n",
    "\n",
    "assert t.allclose(l, X @ W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Compute the log-probability for all models, and for $y=1$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y=1| \\ell} &= \\sigma\\b{\\ell} = \\frac{1}{1+e^{-\\ell}}\\\\\n",
    "  \\log \\P{y=1| \\ell} &= \\log \\sigma\\b{\\ell} = -\\log \\b{1+e^{-\\ell}}.\n",
    "\\end{align}\n",
    "\n",
    "For $y=0$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y=0| \\ell} &= 1 - \\sigma\\b{\\ell}\\\\\n",
    "   &= 1 -\\frac{1}{1+e^{-\\ell}}\\\\\n",
    "   &= \\frac{1+e^{-\\ell} -1}{1+e^{-\\ell}}\\\\\n",
    "   &= \\frac{e^{-\\ell}}{1+e^{-\\ell}}\\\\\n",
    "   &= \\frac{1}{1+e^{\\ell}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.2636e-01, -9.1636e-01, -9.1142e-04])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp = t.zeros(3)\n",
    "lp[0] = -t.log(1+t.exp( l[0,0])) +\\\n",
    "        -t.log(1+t.exp( l[1,0])) +\\\n",
    "        -t.log(1+t.exp(-l[2,0])) +\\\n",
    "        -t.log(1+t.exp(-l[3,0]))\n",
    "lp[1] = -t.log(1+t.exp( l[0,1])) +\\\n",
    "        -t.log(1+t.exp( l[1,1])) +\\\n",
    "        -t.log(1+t.exp(-l[2,1])) +\\\n",
    "        -t.log(1+t.exp(-l[3,1]))\n",
    "lp[2] = -t.log(1+t.exp( l[0,2])) +\\\n",
    "        -t.log(1+t.exp( l[1,2])) +\\\n",
    "        -t.log(1+t.exp(-l[2,2])) +\\\n",
    "        -t.log(1+t.exp(-l[3,2]))\n",
    "assert t.allclose(lp, Bernoulli(logits=l).log_prob(y).sum(0))\n",
    "lp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The model with the largest log-probability is model 3.  (Note that in the discrete case, probabilities can't be bigger than $1$, so log-probabilities must be smaller than zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 2: Classification: maximum a-posteriori </h3>\n",
    "    \n",
    "Using the models and test-set given in Q1, use the log-joint probability as the objective to find the best model.  Use the following prior over the weights,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\w} &= \\N{\\w; \\0, \\I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Compute the log-prior for each model\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\w} &= -\\tfrac{1}{2} \\sum_i w_i^2 + \\text{const}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  -1.0000,   -8.0000, -100.0000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prior = t.zeros(3)\n",
    "log_prior[0] = -0.5*(1**2  + 1**2)\n",
    "log_prior[1] = -0.5*(4**2  + 0.01**2)\n",
    "log_prior[2] = -0.5*(10**2 + 10**2)\n",
    "\n",
    "assert t.allclose(log_prior, -0.5*(W**2).sum(0))\n",
    "\n",
    "log_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Now, compute the log-joint as the sum of the log-prior and log-likelihood,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  -1.4264,   -8.9164, -100.0009])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_joint = t.zeros(3)\n",
    "log_joint[0] = log_prior[0] + lp[0]\n",
    "log_joint[1] = log_prior[1] + lp[1]\n",
    "log_joint[2] = log_prior[2] + lp[2]\n",
    "\n",
    "assert t.allclose(log_joint, log_prior + lp)\n",
    "\n",
    "log_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 3: Classification: test-error </h3>\n",
    "    \n",
    "Using the models and test-set given in Q1, use classification error to find the one model that performs worse than the others.  Use a classification boundary of $p=0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Typically, we'd need to compute the actual probabilities here.  But in this specific example, the threshold is $p=0.5$, which corresponds to a logits-threshold of 0.  So to classify datapoints, we just need to see whether the logits-values are above zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -5.3000,  -8.4320, -53.0000],\n",
       "        [ -4.6000, -13.6120, -46.0000],\n",
       "        [  4.8000,   4.8360,  48.0000],\n",
       "        [  0.7000,  -0.3920,   7.0000]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Therefore, the predictions are,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [1, 1, 1],\n",
       "        [1, 0, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0 < l).int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Compared to the original $y$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We see that the second (middle) model makes one error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 4: K-nearest neighbours </h3>\n",
    "\n",
    "Find the predicted class-label for the test point (0.2, -0.3), with $K=3$.  (Using the standard Euclidean distance; note that the data is the same as above). \n",
    "```  \n",
    " x_0    x_1   y\n",
    "-2.1   -3.2   0\n",
    "-3.4   -1.2   0\n",
    "-2.6   -2.7   0\n",
    " 3.2    2.1   1\n",
    " 1.2    3.6   1\n",
    " 0.6    0.0   1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = t.tensor([\n",
    "    [-2.1, -3.2],\n",
    "    [-3.4, -1.2],\n",
    "    [-2.6, -2.7],\n",
    "    [ 3.2,  2.1],\n",
    "    [ 1.2,  3.6],\n",
    "    [-0.6,  0.0]\n",
    "])\n",
    "\n",
    "y = t.tensor([\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [1.],\n",
    "    [1.],\n",
    "    [1.]\n",
    "])\n",
    "\n",
    "test_datapoint = t.tensor([0.2, -0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "First, compute (squared) distances from the test datapoint, $(0.2, -0.3)$, to the training datapoints,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.7000, 13.7700, 13.6000, 14.7600, 16.2100,  0.7300])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2s = t.zeros(6)\n",
    "d2s[0] = (0.2 - (-2.1))**2 + (-0.3 - (-3.2))**2\n",
    "d2s[1] = (0.2 - (-3.4))**2 + (-0.3 - (-1.2))**2\n",
    "d2s[2] = (0.2 - (-2.6))**2 + (-0.3 - (-2.7))**2\n",
    "d2s[3] = (0.2 - ( 3.2))**2 + (-0.3 - ( 2.1))**2\n",
    "d2s[4] = (0.2 - ( 1.2))**2 + (-0.3 - ( 3.6))**2\n",
    "d2s[5] = (0.2 - (-0.6))**2 + (-0.3 - ( 0.0))**2\n",
    "\n",
    "assert t.allclose(d2s, ((test_datapoint-X)**2).sum(1))\n",
    "\n",
    "d2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Smallest distances are for datapoints 0, 2, 5.\n",
    "\n",
    "Therefore, the datapoint should be classified as $y=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 5: Weighted-nearest neighbours </h3>\n",
    "\n",
    "For the dataset and test-point given above, compute the prediction for weighted-nearest neighbours where we use a weighting function of,\n",
    "\n",
    "\\begin{align}\n",
    "  k(\\x, \\y) &= \\exp\\b{-\\tfrac{1}{4}\\sum_i \\b{x_i-y_i}^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the squared distances computed in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0325, 0.0320, 0.0334, 0.0250, 0.0174, 0.8332])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks = t.zeros(6)\n",
    "ks[0] = (-d2s[0]/4).exp()\n",
    "ks[1] = (-d2s[1]/4).exp()\n",
    "ks[2] = (-d2s[2]/4).exp()\n",
    "ks[3] = (-d2s[3]/4).exp()\n",
    "ks[4] = (-d2s[4]/4).exp()\n",
    "ks[5] = (-d2s[5]/4).exp()\n",
    "\n",
    "assert t.allclose(ks, (-d2s/4).exp())\n",
    "\n",
    "ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Remember in the lectures we had the following formula,\n",
    "\n",
    "\\begin{align}\n",
    "  p_{\\lambda} &= \\frac{\\sum_{\\lambda'}k(\\x_\\lambda, \\x_{\\lambda'}) \\delta_{1, y_{\\lambda'}}}{\\sum_{\\lambda''} k(\\x_\\lambda, \\x_{\\lambda''})}\n",
    "\\end{align}\n",
    "\n",
    "But as the denominator doesn't depend on the class, we just need the numerator, and this amounts to summing the kernels for datapoints belonging to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0979), tensor(0.8755))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_class_0 = ks[0] + ks[1] + ks[2]\n",
    "sum_class_1 = ks[3] + ks[4] + ks[5]\n",
    "(sum_class_0, sum_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "So this weighted-nearest neighbours algorithm classifies the test-point as $y=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 6: Bayesian classification </h3>\n",
    "\n",
    "Given training data \n",
    "```  \n",
    " x_0   y\n",
    "-2.1   0\n",
    "-3.4   0\n",
    "-2.6   0\n",
    " 3.2   1\n",
    " 1.2   1\n",
    " 0.6   1\n",
    "```\n",
    "\n",
    "Fit Gaussian distributions to each cluster using maximum-likelihood, then compute the corresponding posterior over the class-label for $x_0=-0.3$ (specifically $\\P{y=1| x_0=-0.3}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Begin by fitting distributions for each class,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2.6999999999999997, 0.5354126134736357)\n",
      "(1.6666666666666667, 1.1115554667022045)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#means E[x_0]\n",
    "m0 = ((-2.1) + (-3.4) + (-2.6))/3\n",
    "m1 = (( 3.2) + ( 1.2) + ( 0.6))/3\n",
    "\n",
    "#E[x_0^2]\n",
    "Ex2_0 = ((-2.1)**2 + (-3.4)**2 + (-2.6)**2)/3\n",
    "Ex2_1 = (( 3.2)**2 + ( 1.2)**2 + ( 0.6)**2)/3\n",
    "\n",
    "#variances, Var[x_0]\n",
    "\n",
    "v0 = Ex2_0 - m0**2\n",
    "v1 = Ex2_1 - m1**2\n",
    "\n",
    "#standard deviations\n",
    "\n",
    "s0 = math.sqrt(v0)\n",
    "s1 = math.sqrt(v1)\n",
    "\n",
    "#sanity check\n",
    "print((m0, s0))\n",
    "print((m1, s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.229065876417563e-05, 0.07502778775431443)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probability of test-point under each class distribution, P(x| y)\n",
    "#test point:\n",
    "x = -0.3\n",
    "\n",
    "p_like_0 = math.exp(-(x-m0)**2/(2*v0))/math.sqrt(2*math.pi*v0)\n",
    "p_like_1 = math.exp(-(x-m1)**2/(2*v1))/math.sqrt(2*math.pi*v1)\n",
    "\n",
    "assert t.allclose(t.tensor([p_like_0]), Normal(m0, s0).log_prob(x).exp())\n",
    "assert t.allclose(t.tensor([p_like_1]), Normal(m1, s1).log_prob(x).exp())\n",
    "\n",
    "(p_like_0, p_like_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We have just obtained $\\P{x| y=0}$ and $\\P{x| y=1}$.  To convert this into the posterior, $\\P{y=1| x}$, we need to apply Bayes theorem,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y| x} &= \\frac{\\P{x| y} \\P{y}}{\\P{x}}\n",
    "\\end{align}\n",
    "\n",
    "But as the denominator doesn't depend on $y$, and the prior is uniform, we have,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y| x} &\\propto \\P{x| y}\n",
    "\\end{align}\n",
    "\n",
    "Thus, we just need to normalise the probabilities computed above, such that they sum to one,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995698024909264"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_post_1 = p_like_1/(p_like_0+p_like_1)\n",
    "p_post_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "So, $\\P{y=1| x} = 0.9996$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 7: Classification vs clustering </h3>\n",
    "\n",
    "Which of these cluster-assignments are equivalent?\n",
    "\n",
    "```\n",
    "y_0   y_1   y_2   y_3\n",
    " 0     1     2     0\n",
    " 1     0     0     1\n",
    " 2     2     1     2\n",
    " 1     0     0     1\n",
    " 2     2     1     2\n",
    " 0     1     2     0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Ans: all of them are equivalent (because they all group the same datapoints together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 7: KNN M-step </h3>\n",
    "\n",
    "For the following data-points, and cluster assignments, $z$, perform one M-step to find the corresponding cluster-centers.\n",
    "\n",
    "```  \n",
    " x_0    x_1   z\n",
    "-2.1   -3.2   1\n",
    "-3.4   -1.2   0\n",
    "-2.6   -2.7   1\n",
    " 3.2    2.1   0\n",
    " 1.2    3.6   1\n",
    " 0.6    0.0   0\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.1667, -0.7667]), tensor([-0.2667,  0.3000]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean for cluster 0\n",
    "m0 = t.zeros(2)\n",
    "m0[0] = ((-2.1) + (-2.6) + (1.2))/3\n",
    "m0[1] = ((-3.2) + (-2.7) + (3.6))/3\n",
    "#mean for cluster 1\n",
    "m1 = t.zeros(2)\n",
    "m1[0] = ((-3.4) + ( 3.2) + (-0.6))/3\n",
    "m1[1] = ((-1.2) + ( 2.1) + ( 0.0))/3\n",
    "(m0, m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 8: KNN E-step </h3>\n",
    "\n",
    "Given cluster centers in the previous question, update the cluster assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be fairly obvious that cluster 0 is closer to datapoints 0, 1, 2, and cluster 1 is closer to datapoints 3 and 4.\n",
    "\n",
    "For datapoint 5, we should probably do the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = t.tensor([\n",
    "    [-2.1, -3.2],\n",
    "    [-3.4, -1.2],\n",
    "    [-2.6, -2.7],\n",
    "    [ 3.2,  2.1],\n",
    "    [ 1.2,  3.6],\n",
    "    [-0.6,  0.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9089, 0.2011])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = t.zeros(2)\n",
    "ds[0] = (X[5, 0] - m0[0])**2 + (X[5, 1] - m0[1])**2\n",
    "ds[1] = (X[5, 0] - m1[0])**2 + (X[5, 1] - m1[1])**2\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "So the fifth datapoint should be assigned to cluster 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
