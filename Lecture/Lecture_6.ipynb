{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import Normal, Categorical, Bernoulli\n",
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual\n",
    "#elliptical cluster shapes with wrong classification boundary to motivate GMM\n",
    "#introduce q in Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\Categorical}{{\\rm Categorical}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}[1][]{{\\rm P}_{#1}\\!\\b}\n",
    "\\newcommand{\\Q}[1][]{{\\rm Q}_{#1}\\!\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}[1][]{\\mathbb{E}_{#1}\\!\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "\\newcommand{\\p}{\\mathbf{p}}\n",
    "\\newcommand{\\q}{\\mathbf{q}}\n",
    "\\newcommand{\\l}{\\boldsymbol{\\ell}}\n",
    "\\DeclareMathOperator{\\softmax}{softmax}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\norm}{\\bracket{\\lVert}{\\rVert}}\n",
    "\\newcommand{\\Dkl}[2]{D_\\text{KL} \\left( #1 \\middle\\Vert #2 \\right)}\n",
    "$$\n",
    "\n",
    "<h1> Lecture 6: Unsupervised learning and clustering </h1>\n",
    "\n",
    "<h2> Clustering is not classification... </h2>\n",
    "<h2>...and unsupervised learning is not supervised learning </h2>\n",
    "Clustering is a type of unsupervised learning, which is very, very different from the supervised learning you have seen in my slides so far.\n",
    "\n",
    "In supervised learning, we have a list of input, `x`, output, `y`, pairs as data, and the idea is to learn a function that maps from a new input test point, to a distribution over the corresonding `Y`\n",
    "\n",
    "```\n",
    "#### Supervised learning\n",
    "x : X # Input\n",
    "y : Y # Output\n",
    "    [(X, Y)] -> (X -> Y) \n",
    "    [(X, Y)] -> (X -> Distribution{Y}) \n",
    "```\n",
    "\n",
    "In unsupervised learning/clustering, we only have a list of inputs, and the goal is to compute a (distribution over) a latent variable that somehow summarises the structure of the inputs.\n",
    "\n",
    "```\n",
    "#### Unsupervised learning\n",
    "x : X # Input\n",
    "z : Z # Latent\n",
    "    [X] -> [Z]\n",
    "    [X] -> [Distribution{Z}]\n",
    "```\n",
    "\n",
    "<h2> First example </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a434a569974f838a4a4fd2b7e0a700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "Z = t.cat([\n",
    "    t.zeros(50).int(),\n",
    "    t.ones(50).int()\n",
    "])\n",
    "\n",
    "#fig = plot.figure()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(7,3), sharey=True)\n",
    "axs[0].set_xlim(-2, 2)\n",
    "axs[1].set_xlim(-2, 2)\n",
    "axs[2].set_xlim(-2, 2)\n",
    "axs[0].set_ylim(-2, 2)\n",
    "axs[0].scatter(X[:, 0], X[:, 1])\n",
    "axs[1].scatter(X[:, 0], X[:, 1], c=Z);\n",
    "axs[2].scatter(X[:, 0], X[:, 1], c=-Z);\n",
    "axs[0].set_title(\"original data, without labels\")\n",
    "axs[1].set_title(\"clustered data\")\n",
    "axs[2].set_title(\"equiv. clustering\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Many classification problems ignore clusters: example </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f8fd7065444c1c8e3e3b44ea9c3f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = Bernoulli(t.sigmoid(100*X[:, 1])).sample()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_xlabel(\"height\")\n",
    "ax.set_ylabel(\"A-level scores\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Simplest clustering algorithm: K-means </h2>\n",
    "\n",
    "We introduce parameters, $\\m$, and latent variables, $z_\\lambda$, associated with each datapoint.  The parameters, $\\m$ describe the central points of $K$ clusters, and the latents assign each datapoint to a cluster.  Thus, we have three/four key objects,\n",
    "```python\n",
    "# N == number of datapoints\n",
    "# K == number of clusters\n",
    "# D == dimension of the data vectors (usually 2 in our examples)\n",
    "X.shape  == (N, 1, D) # Data\n",
    "mu.shape ==    (K, D) # Cluster-centers\n",
    "q.shape  == (N, K, 1) # One-hot representation of the cluster-assignments for each datapoint\n",
    "z.shape  == (N,)      # Cluster index for each datapoint\n",
    "```\n",
    "\n",
    "The objective is to minimise the squared distance between each datapoint and its assigned cluster center,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(\\z, \\m) &= \\sum_\\lambda \\norm{\\x_\\lambda - \\m_{z_\\lambda}}^2\n",
    "\\end{align}\n",
    "\n",
    "To optimize this algorithm, we use coordinate descent (i.e. we alternate between optimizing $\\z$ and $\\m$).\n",
    "\n",
    "First, we assign each datapoint to the nearest cluster,\n",
    "\n",
    "\\begin{align}\n",
    "  z_\\lambda &\\rightarrow \\argmin_{k\\in\\{1...K\\}} \\norm{\\x_\\lambda - \\m_k}^2\n",
    "\\end{align}\n",
    "\n",
    "Then, we put the cluster centers at the mean of the assigned datapoints,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m_k &\\rightarrow \\frac{1}{\\sum_\\lambda \\delta_{k, z_\\lambda}} \\sum_{\\lambda} \\delta_{k, z_\\lambda} \\x_\\lambda\n",
    "\\end{align}\n",
    "\n",
    "remember that\n",
    "\n",
    "\\begin{align}\n",
    "  \\delta_{k, z_\\lambda} = \n",
    "  \\begin{cases}\n",
    "    1 &\\text{if $k=z_\\lambda$, i.e. datapoint $\\lambda$ is currently assigned to the $k$th cluster}\\\\\n",
    "    0 &\\text{if $k\\neq z_\\lambda$, i.e. datapoint $\\lambda$ is not in the $k$th cluster}\n",
    "  \\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a047a9115eb94da5b9dc32a0bf142e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.manual_seed(0)\n",
    "\n",
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5,\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "def kmeans(X, K):\n",
    "    N, D = X.shape\n",
    "    X = X[:, None, :]\n",
    "    mu = t.randn(K, D)\n",
    "    while True:\n",
    "        sd = ((X - mu)**2).sum(-1) # sd.shape = (N, K)\n",
    "        z = t.argmin(sd, 1)        # z.shape  = (N)\n",
    "        q = t.zeros(N, K, 1)\n",
    "        q[t.arange(N), z, 0] = 1.\n",
    "        print(f\"loss = {loss(X, z, mu).item()}\")\n",
    "        plot(X, z, mu)\n",
    "        yield None\n",
    "        \n",
    "        mu = (q * X).sum(0) / q.sum(0)\n",
    "        print(f\"loss = {loss(X, z, mu).item()}\")\n",
    "        plot(X, z, mu)\n",
    "        yield None\n",
    "        \n",
    "def loss(X, z, mu):\n",
    "    return ((X - mu[z, :][:, None, :])**2).mean()\n",
    "    \n",
    "def plot(X, z, mu):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X[:, 0, 0], X[:, 0, 1], c=z);\n",
    "    ax.scatter(mu[:, 0], mu[:, 1], s=100, c='r', label=\"cluster centers\")\n",
    "    ax.legend()\n",
    "    \n",
    "kmeans_iter = iter(kmeans(X, 3))\n",
    "def kmeans_call():\n",
    "    next(kmeans_iter)\n",
    "interact_manual(kmeans_call);\n",
    "\n",
    "#Change marker type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one strange property of this algorithm is that it assumes \"hard\" cluster assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Soft clustering and the Gaussian Mixture Model (GMM) </h2>\n",
    "\n",
    "The goal of the GMM, is really just to model the density of the data we use the latent variables to give us a better model of the data.\n",
    "\n",
    "For instance, consider fitting a MvNormal to the following data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logPx = -127.18228149414062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483389f44c614805bda28eaa2a881b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "N = X.shape[0]\n",
    "mu = X.sum(0)/N\n",
    "cov = ((X - mu).T @ (X-mu))/N\n",
    "mvn = MvNormal(mu, cov)\n",
    "\n",
    "logPx = mvn.log_prob(X).sum()\n",
    "print(f\"logPx = {logPx}\")\n",
    "\n",
    "P = 100\n",
    "x0s = t.linspace(-2, 2, P)[:, None].expand(P, P)\n",
    "x1s = x0s.T\n",
    "xs  = t.stack([x0s.reshape(-1), x1s.reshape(-1)], 1)\n",
    "ps = mvn.log_prob(xs).exp().view(P, P)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.scatter(X[:, 0], X[:, 1]);\n",
    "ax.contour(x0s, x1s, ps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get a better model for this kind of data?  We use a GMM.\n",
    "\n",
    "The simplest form of a GMM is a density, with $K$ different Gaussians, in different locations, $\\m_k$, and with different covariance matrices, $\\S_k$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{x_\\lambda} &= \\sum_{k=1}^K p_k \\N{\\x_\\lambda; \\m_k, \\S_k}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this doesn't give us a way to extract any notion of assignment of a datapoint to a mixture component.\n",
    "\n",
    "We therefore write this distribution in terms of a latent variable,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_\\lambda} &= \\sum_{z_\\lambda=1}^K \\P{\\x_\\lambda| z_\\lambda} \\P{z_\\lambda}\n",
    "\\end{align}\n",
    "\n",
    "where, $\\P{z_\\lambda}$ is a Categorical, and represents the mixture component for the $\\lambda$th data point, and $\\P{x_\\lambda| z_\\lambda}$ is a _single_ Gaussian, (corresponding to the $z_\\lambda$th mixture component),\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{z_\\lambda} &= \\Categorical{z_\\lambda; \\p} = p_{z_\\lambda}\\\\\n",
    "  \\P{\\x_\\lambda| z_\\lambda} &= \\N{\\x_\\lambda; \\m_{z_\\lambda}, \\S_{z_\\lambda}}.\n",
    "\\end{align}\n",
    "\n",
    "The substituting in the values of $\\P{z_\\lambda}$ and $\\P{x_\\lambda| z_\\lambda}$, we get the same expression as above, except use $z_\\lambda$ as the summation index, instead of $k$\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_\\lambda} &= \\sum_{z_\\lambda=1}^K \\P{\\x_\\lambda| z_\\lambda} \\P{z_\\lambda} = \\sum_{z_\\lambda=1}^K p_{z_\\lambda} \\N{\\x_\\lambda; \\m_{z_\\lambda}, \\S_{z_\\lambda}}.\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use Bayes theorem to give a posterior distribution over $z_\\lambda$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{z_\\lambda| \\x_\\lambda} &\\propto \\P{\\x_\\lambda| z_\\lambda} \\P{z_\\lambda}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggests an algorithm similar to K-means: first compute the posterior over $z_\\lambda$ (E-step), then update the parameters of the Gaussians, $\\m_k$ and $\\S_k$, using a mean weighted by the posterior (M-step).\n",
    "\n",
    "This algorith is known as expectation-maximization (EM).  Computing the posterior is known as the E-step, and updating the parameters is the M-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757046eb8d754975ab0f4b34d0abbcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.manual_seed(0)\n",
    "\n",
    "X = t.cat([\n",
    "    t.randn(50, 2)/3 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/3 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "def gmm_em(X, K):\n",
    "    N, D = X.shape\n",
    "    X = X[:, None, :]\n",
    "    mu  = t.randn(K, D)\n",
    "    cov = 0.5*t.eye(D, D).expand(K, -1, -1)\n",
    "    while True:\n",
    "        # unnormalized posterior probability\n",
    "        q = MvNormal(mu, cov).log_prob(X).exp()\n",
    "        # normalized posterior probability\n",
    "        q = q / q.sum(1, keepdim=True)\n",
    "        # expand out last such that q.shape = (N, K, 1)\n",
    "        q = q[:, :,  None]\n",
    "        plot_gmm(X, q, mu, cov)\n",
    "        yield None\n",
    "        \n",
    "        #weighted mean\n",
    "        mu = (q * X).sum(0) / q.sum(0)\n",
    "        plot_gmm(X, q, mu, cov)\n",
    "        yield None\n",
    "\n",
    "        \n",
    "def plot_gmm(X, q, mu, cov):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X[:, 0, 0], X[:, 0, 1], c=q[:, 0, 0]);\n",
    "    ax.scatter(mu[:, 0], mu[:, 1], s=100, c='r', label=\"cluster centers\")\n",
    "    ax.legend()\n",
    "\n",
    "    \n",
    "gmm_em_iter = iter(gmm_em(X, 2))\n",
    "def gmm_em_call():\n",
    "    next(gmm_em_iter)\n",
    "interact_manual(gmm_em_call);\n",
    "\n",
    "#Change marker type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the objective function in EM?\n",
    "\n",
    "Turns out that this is a very subtle question.\n",
    "\n",
    "We define an \"approximate posterior\", corresponding to the current cluster assignments,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Q[\\q]{z_\\lambda} &= \\Categorical{z_\\lambda; \\q_{\\lambda}}\n",
    "\\end{align}\n",
    "\n",
    "We can write down the \"evidence lower-bound objective\" (ELBO).  Note that the \"model evidence\" is $\\log \\P{\\x}$, and the ELBO can be defined as,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P[\\m]{\\x} \\geq \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x}- \\Dkl{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}\n",
    "\\end{align}\n",
    "\n",
    "Where the ELBO is a lower-bound because the KL-divergence is non-negative,\n",
    "\n",
    "\\begin{align}\n",
    "  0 \\leq \\Dkl{\\Q[q]{\\z}}{\\P[\\m]{\\z| \\x}} = \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}}\n",
    "\\end{align}\n",
    "\n",
    "And the KL-divergence equals zero when the approximate posterior equals the true posterior, $\\Q{\\z} = \\P{\\z| \\x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Expectation (E) step </h3>\n",
    "\n",
    "Thus, the E-step consists of maximizing the ELBO wrt to the parameters of the approximate posterior, $\\q$.\n",
    "\n",
    "\\begin{align}\n",
    "  \\q \\rightarrow \\argmax_{\\q} \\L\\b{\\m, \\q}\n",
    "\\end{align}\n",
    "\n",
    "as $\\log \\P[\\m]{\\x}$ does not depend on $\\q$ this is equivalent to,\n",
    "\n",
    "\\begin{align}\n",
    "  \\q \\rightarrow \\argmax_{\\q} \\Dkl{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}} \n",
    "\\end{align}\n",
    "\n",
    "and because the KL-divergence is minimized by setting $\\Q{z}$ equal to the true posterior, we need,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Q{\\z} \\rightarrow \\P{\\z| \\x}\n",
    "\\end{align}\n",
    "\n",
    "concretely, we can achieve this by setting,\n",
    "\n",
    "\\begin{align}\n",
    "  q_{\\lambda, z_\\lambda} \\rightarrow \\P{z_{\\lambda}| \\x_\\lambda}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<h3> Maximization (M) step </h3>\n",
    "\n",
    "The goal is,\n",
    "\\begin{align}\n",
    "  \\m \\rightarrow \\argmax_{\\m} \\L\\b{\\m, \\q}\n",
    "\\end{align}\n",
    "\n",
    "Both terms in the previous form for the ELBO depend on $\\m$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x} + \\Dkl{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}\n",
    "\\end{align}\n",
    "\n",
    "so we need to rearrange to get a good update for $\\m$.\n",
    "\n",
    "In particular,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x} - \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x} + \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\P[\\m]{\\z| \\x}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\P[\\m]{\\x} + \\log \\frac{\\P[\\m]{\\z| \\x}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\P[\\m]{\\z| \\x} \\P[\\m]{\\x}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\P[\\m]{\\x, \\z}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\P[\\m]{\\x, \\z}} - \\E[{\\Q[\\q]{\\z}}]{\\Q[\\q]{\\z}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that the second term doesn't depend on $\\m$, so,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m \\rightarrow \\argmax_{\\m} \\E[{\\Q[\\q]{\\z}}]{\\log \\P[\\m]{\\x, \\z}}\n",
    "\\end{align}\n",
    "\n",
    "And this is just maximum-likelihood fitting of the parameters of the mixtures (here, the mixture-means), with datapoints weighted by $\\Q{\\z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
