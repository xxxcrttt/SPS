{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "$$\n",
    "\n",
    "<h1> Question sheet 1: maximum likelihood regression </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Question 1 </h2>\n",
    "\n",
    "Derive the regularised maximum likelihood solution to the following optimization problem,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(\\w) &= \\log \\P{\\y| \\X, \\w} - \\tfrac{1}{2} \\w^T \\La \\w\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Answer </h3>\n",
    "\n",
    "We begin by taking the gradient of $\\log \\P{\\y| \\X, \\w}$ from the notes,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\log \\P{\\y| \\X, \\w}]{\\w} &= \\tfrac{1}{\\sigma^2} \\X^T \\b{\\y - \\X \\w}\n",
    "\\end{align}\n",
    "\n",
    "Next, we consider the gradient of the second term,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd{w_\\alpha} \\sb{-\\tfrac{1}{2} \\sum_{ij} w_i \\Lambda_{ij} w_j} \n",
    "  &= -\\tfrac{1}{2} \\sb{\\sum_{ij} \\dd[w_i]{w_\\alpha} \\Lambda_{ij} w_j + \\sum_{ij} w_i \\Lambda_{ij} \\dd[w_j]{w_\\alpha}}\\\\\n",
    "  &= -\\tfrac{1}{2} \\sb{\\sum_{j} \\Lambda_{\\alpha j} w_j + \\sum_{i} w_i \\Lambda_{i \\alpha}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "as $\\La$ is symmetric,\n",
    "\n",
    "\\begin{align}\n",
    "  &= -\\tfrac{1}{2} \\sb{\\sum_{j} \\Lambda_{\\alpha j} w_j + \\sum_{i} w_i \\Lambda_{\\alpha i}}\\\\\n",
    "  &= - \\sum_{i} \\Lambda_{\\alpha i} w_i\n",
    "\\end{align}\n",
    "\n",
    "Putting everything back in matrix notation,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd{\\w} \\sb{ - \\tfrac{1}{2} \\w^T \\La \\w} &= - \\La \\w\n",
    "\\end{align}\n",
    "\n",
    "Combining the first and second terms, we can compute the gradient of the objective,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L(\\w)]{\\w} &= \\tfrac{1}{\\sigma^2} \\X^T \\b{\\y - \\X \\w} - \\La \\w.\n",
    "\\end{align}\n",
    "\n",
    "Finally, we solve for the location, $\\wh$, where this gradient is zero,\n",
    "\n",
    "\\begin{align}\n",
    "  \\0 &= \\X^T \\b{\\y - \\X \\wh} - \\La \\wh\\\\\n",
    "  \\0 &= \\X^T \\b{\\y - \\X \\wh} - \\sigma^2 \\La \\wh\\\\\n",
    "  \\0 &= \\X^T \\y - \\X^T \\X \\wh - \\sigma^2 \\La \\wh\\\\\n",
    "  \\0 &= \\X^T \\y - \\b{\\X^T \\X + \\sigma^2 \\La} \\wh\\\\\n",
    "  \\b{\\X^T \\X + \\sigma^2 \\La} \\wh &= \\X^T \\y \\\\\n",
    "  \\wh &= \\b{\\X^T \\X + \\sigma^2 \\La}^{-1} \\X^T \\y \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Question 2 </h2>\n",
    "\n",
    "For the data sample in the table, and a model of the form $y = w_0 + w_1 x$, a noise-level of $\\sigma = 1$, and a regulariser, $\\La = 2 \\I$, compute the regularised ML solution.\n",
    "\\begin{align}\n",
    "  \\L\\b{\\w} &= \\log \\N{\\y; \\X \\w, \\sigma^2} - \\tfrac{1}{2}\\w^T \\La \\w\n",
    "\\end{align}\n",
    "\n",
    "\\begin{tabular}{rr}\n",
    "  x   & y   \\\\\n",
    "  \\hline\n",
    "   -2.0 &  -6.2 \\\\\n",
    "   -1.0 &  -2.6 \\\\\n",
    "    0.0 &   0.5 \\\\\n",
    "    1.0 &   2.7 \\\\\n",
    "    2.0 &   5.7\n",
    "\\end{tabular}\n",
    "\n",
    "Do this using a calculator, as if you were in an exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Answer </h3>\n",
    "\n",
    "First, write down $\\X$, $\\y$, $\\La$ and $\\sigma$ for error-checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.tensor([\n",
    "    [1., -2.],\n",
    "    [1., -1.],\n",
    "    [1.,  0.],\n",
    "    [1.,  1.],\n",
    "    [1.,  2.]\n",
    "])\n",
    "\n",
    "y = t.tensor([\n",
    "    [-6.2],\n",
    "    [-2.6],\n",
    "    [ 0.5],\n",
    "    [ 2.7],\n",
    "    [ 5.7]\n",
    "])\n",
    "\n",
    "La = 2*t.eye(2)\n",
    "s2 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by computing $\\X^T \\X$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  0.],\n",
       "        [ 0., 10.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTX = t.zeros(2,2)\n",
    "\n",
    "XTX[0,0] = ( 1.)**2 + ( 1.)**2 + ( 1.)**2 + ( 1.)**2 + ( 1.)**2\n",
    "XTX[1,1] = (-2.)**2 + (-1.)**2 + ( 0.)**2 + ( 1.)**2 + ( 2.)**2\n",
    "XTX[0,1] = 1.*(-2.) + 1.*(-1.) + 1.*( 0.) + 1.*( 1.) + 1.*( 2.)\n",
    "XTX[1,0] = XTX[0,1]\n",
    "\n",
    "assert t.allclose(XTX, X.T@X)\n",
    "XTX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next compute, $\\X^T \\X + \\sigma^2 \\La$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  0.],\n",
       "        [ 0., 12.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTX_s2La = t.zeros(2,2)\n",
    "\n",
    "XTX_s2La[0,0] = XTX[0,0] + s2*2\n",
    "XTX_s2La[1,1] = XTX[1,1] + s2*2\n",
    "XTX_s2La[1,0] = XTX[1,0]\n",
    "XTX_s2La[0,1] = XTX[0,1]\n",
    "\n",
    "assert t.allclose(XTX_s2La, X.T@X + s2*La)\n",
    "XTX_s2La"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute $\\b{\\X^T \\X + \\sigma^2 \\La}^{-1}$ inverse using the standard formula,\n",
    "\n",
    "\\begin{align}\n",
    "  \\begin{pmatrix} a &  b \\\\  c & d \\end{pmatrix}^{-1} &=\n",
    "  \\frac{1}{ad-bc}\n",
    "  \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(84.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1429, -0.0000],\n",
       "        [-0.0000, 0.0833]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_XTX_s2La = t.zeros(2,2)\n",
    "\n",
    "det = XTX_s2La[0,0]*XTX_s2La[1,1] - XTX_s2La[1,0]*XTX_s2La[0,1]\n",
    "print(det)\n",
    "\n",
    "inv_XTX_s2La[0,0] =  XTX_s2La[1,1]/det\n",
    "inv_XTX_s2La[1,1] =  XTX_s2La[0,0]/det\n",
    "inv_XTX_s2La[1,0] = -XTX_s2La[1,0]/det\n",
    "inv_XTX_s2La[0,1] = -XTX_s2La[0,1]/det\n",
    "\n",
    "assert t.allclose(inv_XTX_s2La, t.inverse(X.T@X + s2*La))\n",
    "inv_XTX_s2La"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute $\\X^T \\y$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1000],\n",
       "        [29.1000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTy = t.zeros(2, 1)\n",
    "\n",
    "XTy[0,0] = ( 1.)*(-6.2) + ( 1.)*(-2.6) + ( 1.)*(0.5) + ( 1.)*(2.7) + ( 1.)*(5.7)\n",
    "XTy[1,0] = (-2.)*(-6.2) + (-1.)*(-2.6) + ( 0.)*(0.5) + ( 1.)*(2.7) + ( 2.)*(5.7)\n",
    "\n",
    "assert t.allclose(XTy, X.T@y)\n",
    "XTy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute $\\b{\\X^T \\X + \\sigma^2 \\La}^{-1} \\X^T \\y$ as a matrix-vector multiplication,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0143],\n",
       "        [2.4250]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh = t.zeros(2, 1)\n",
    "\n",
    "wh[0,0] = inv_XTX_s2La[0,0] * XTy[0,0] + inv_XTX_s2La[0,1] * XTy[1,0]\n",
    "wh[1,0] = inv_XTX_s2La[1,0] * XTy[0,0] + inv_XTX_s2La[1,1] * XTy[1,0]\n",
    "\n",
    "assert t.allclose(wh, t.inverse(X.T@X + s2*La) @ X.T@y)\n",
    "wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
